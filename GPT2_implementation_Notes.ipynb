{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0srZHT12-A9v",
        "7gGd5IT7DlWm"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U\n",
        "!pip install numpy==2.0.1\n",
        "\n",
        "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0\n",
        "!pip install dataclasses==0.6"
      ],
      "metadata": {
        "id": "-_d99aNx0oK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r8gjIVSh-z7"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Reference code for GPT-2 training and inference.\n",
        "Will save the model weights into files, to be read from C as initialization.\n",
        "\n",
        "References:\n",
        "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
        "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
        "2) huggingface/transformers PyTorch implementation:\n",
        "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
        "\n",
        "Example launches to only benchmark the speed of bfloat16 compiled GPU training:\n",
        "1 GPU:\n",
        "python train_gpt2.py --write_tensors=0 --num_iterations=50 --sequence_length=1024 --compile=1 --tensorcores=1 --dtype=bfloat16\n",
        "you can also turn on flash-attention by appending --flash=1\n",
        "4 GPU:\n",
        "torchrun --standalone --nproc_per_node=4 train_gpt2.py --write_tensors=0 --num_iterations=50 --sequence_length=1024 --compile=1 --tensorcores=1 --dtype=bfloat16\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import glob\n",
        "import struct\n",
        "import inspect\n",
        "from contextlib import nullcontext\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch._inductor.config as config\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "from torch.distributed.optim import ZeroRedundancyOptimizer\n",
        "import torch.distributed as dist"
      ],
      "metadata": {
        "id": "wCn2Wi310osP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# PyTorch nn.Module definitions for the GPT-2 model"
      ],
      "metadata": {
        "id": "fM6pPFCw1lFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NewGELU(nn.Module):\n",
        "    \"\"\"Careful there are a few versions of GeLU, this one is the exact one used by OpenAI\"\"\" # defines GeLU activation function, tanh version.\n",
        "    def forward(self, input):\n",
        "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))"
      ],
      "metadata": {
        "id": "4lJcBc4b0oqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using a global to toggle flash-attention\n",
        "\n",
        "# The global variable FLASH is set to 0, which means that by default, the custom flash-attention mechanism is turned off. Flash attention is a type of efficient\n",
        "# attention mechanism that can handle longer sequences more effectively by reducing memory consumption and computational requirements. This would be toggled or\n",
        "# used in specific parts of the attention calculations in your model depending on the requirements or experiments you are conducting.\n",
        "FLASH = 0"
      ],
      "metadata": {
        "id": "fzZ84Pbc0oov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TIL notes for self attention"
      ],
      "metadata": {
        "id": "0srZHT12-A9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Here’s a brief overview of embeddings and attention heads, their roles in the model, and why they are used:\n",
        "\n",
        "### Embeddings\n",
        "**What are they?**\n",
        "Embeddings are dense vector representations of data, typically words or tokens, that capture semantic meanings and relationships in a way that is usable by machine learning models. In the context of models like GPT-2, embeddings transform discrete input tokens (e.g., words) into continuous vectors.\n",
        "\n",
        "**Where are they in the pipeline?**\n",
        "In transformer models, embeddings are usually the first layer. For GPT-2, this includes:\n",
        "- **Token Embeddings (`wte`)**: Convert each token into a vector.\n",
        "- **Positional Embeddings (`wpe`)**: Encode the position of each token in the sequence, allowing the model to understand the order of tokens.\n",
        "\n",
        "**What do they do and why?**\n",
        "Embeddings map tokens to a high-dimensional space where similar tokens are closer together, facilitating the model's understanding of language patterns and context. The positional embeddings ensure the model can consider the order of words, crucial for generating coherent text and understanding language structure.\n",
        "\n",
        "### Attention Heads\n",
        "**What are they?**\n",
        "Attention heads are components of the attention mechanism within each transformer layer. They allow the model to focus on different parts of the input sequence when predicting each output token.\n",
        "\n",
        "**Where are they in the pipeline?**\n",
        "Attention heads are part of the self-attention layers in the transformer blocks. Each head computes its own attention scores independently, allowing the model to capture various aspects of the data.\n",
        "\n",
        "**What do they do and why?**\n",
        "Each attention head:\n",
        "- Computes **queries**, **keys**, and **values** from the input embeddings using linear transformations.\n",
        "- Uses the **queries** to assess the importance (or relevance) of other parts of the sequence (the **keys**) and aggregates the corresponding **values** based on these computed relevancies.\n",
        "\n",
        "This mechanism enables the model to dynamically decide which parts of the input are important for each output token, allowing for contextually aware generation of text. The use of multiple heads increases the model's capacity to focus on different subspaces of the embedding dimension, capturing a wider array of relationships within the data.\n",
        "\n",
        "### Summary\n",
        "In the pipeline, embeddings serve as the initial transformation layer, translating raw input into a format suitable for processing by deeper network layers. Attention heads come into play within the self-attention mechanism, which is fundamental to transformers' ability to handle sequences of data. They allow the model to selectively focus on different segments of the input data, enhancing its ability to make contextually informed predictions."
      ],
      "metadata": {
        "id": "KYZwRcXe-a6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "query: i dont get how theres more embedding splitted across a number of head etc. is this due to gpu parallelism? and i get that embedding = token embd + positional embd.\n",
        "\n",
        "but, how is the tokens splitted across the attention heads? i cant visualise it. can you show me an example of how this works. thx\n",
        "\n",
        "---\n",
        "\n",
        "Certainly! Let’s dive deeper into how embeddings are used within attention heads and the concept of splitting across multiple heads, which can indeed seem a bit abstract at first.\n",
        "\n",
        "### Embeddings Recap\n",
        "As you mentioned, the embedding for each token in a transformer model like GPT-2 is the sum of:\n",
        "- **Token Embedding**: Converts each token (like a word) into a vector.\n",
        "- **Positional Embedding**: Adds information about the position of each token in the sequence.\n",
        "\n",
        "This results in a combined embedding that conveys both the meaning of the token and its position in the sequence.\n",
        "\n",
        "### Attention Heads: Splitting the Embeddings\n",
        "The purpose of having multiple attention heads is to allow the model to attend to different parts of the input sequence from different 'perspectives' or 'subspaces'. Here’s how it works step-by-step:\n",
        "\n",
        "1. **Starting Point**: Assume each token in the sequence has been converted to a combined embedding vector (from token and positional embeddings). Suppose each vector has a dimensionality \\(C\\).\n",
        "\n",
        "2. **Linear Projections**: Each attention head will transform the embedding vector into three different vectors: Queries (Q), Keys (K), and Values (V). These transformations are done through learned linear projections (typically implemented as a single linear layer split into three parts).\n",
        "\n",
        "3. **Splitting Across Heads**:\n",
        "   - For a transformer with multiple heads, the dimension \\(C\\) of each embedding vector is split into \\(N\\) parts, where \\(N\\) is the number of heads. Each part handles a fraction of the total dimension. For instance, if \\(C = 768\\) and \\(N = 12\\) (like in GPT-2), each head deals with a sub-vector of size 64.\n",
        "   - Each head processes its portion of the vector independently. This means every head will produce its own Q, K, and V vectors from different subspaces of the embedding.\n",
        "\n",
        "### Visualization Example\n",
        "Let's visualize this with an example where \\(C = 6\\) (for simplicity) and \\(N = 3\\) heads. Each head will handle 2 dimensions:\n",
        "\n",
        "- **Original Embeddings for a Sequence** (3 tokens):\n",
        "  \\[\n",
        "  \\begin{align*}\n",
        "  \\text{Token 1:} & \\ [e_{11}, e_{12}, e_{13}, e_{14}, e_{15}, e_{16}] \\\\\n",
        "  \\text{Token 2:} & \\ [e_{21}, e_{22}, e_{23}, e_{24}, e_{25}, e_{26}] \\\\\n",
        "  \\text{Token 3:} & \\ [e_{31}, e_{32}, e_{33}, e_{34}, e_{35}, e_{36}]\n",
        "  \\end{align*}\n",
        "  \\]\n",
        "\n",
        "- **Split Across 3 Heads**:\n",
        "  - **Head 1** processes dimensions 1 and 2 of each token's embedding:\n",
        "    - \\( Q_1, K_1, V_1 \\) from \\([e_{11}, e_{12}]\\), \\([e_{21}, e_{22}]\\), \\([e_{31}, e_{32}]\\)\n",
        "  - **Head 2** processes dimensions 3 and 4:\n",
        "    - \\( Q_2, K_2, V_2 \\) from \\([e_{13}, e_{14}]\\), \\([e_{23}, e_{24}]\\), \\([e_{33}, e_{34}]\\)\n",
        "  - **Head 3** processes dimensions 5 and 6:\n",
        "    - \\( Q_3, K_3, V_3 \\) from \\([e_{15}, e_{16}]\\), \\([e_{25}, e_{26}]\\), \\([e_{35}, e_{36}]\\)\n",
        "\n",
        "Each head computes attention independently, focusing on different aspects (or 'slices') of the embedding space. This allows the model to integrate diverse perspectives when synthesizing the output at each step.\n",
        "\n",
        "### GPU Parallelism\n",
        "The splitting across heads is indeed highly conducive to parallel processing on GPUs. Each head operates independently of the others, which means their computations can be parallelized effectively, leading to significant performance gains especially with high-dimensional embeddings and long sequences.\n",
        "\n",
        "This setup not only speeds up computations but also enriches the model's capacity to learn varied dependencies across the input data, leveraging multiple representational strategies simultaneously."
      ],
      "metadata": {
        "id": "4OtNJooh-A7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "query: wait you sure this is real? that the initial token is splited across all 3 instead of Q, K, V all using the same input?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "You've raised a great point for clarification. Let's refine the explanation regarding how the embeddings are processed through the attention mechanism, particularly in the context of dividing the input across multiple heads for computing queries (Q), keys (K), and values (V).\n",
        "\n",
        "### How Embeddings Are Handled in Attention Heads\n",
        "\n",
        "In transformer models like GPT-2, each head does **not** process only a specific part of the input token's embedding vector to produce its own Q, K, and V. Rather, the entire embedding vector is used by each head to generate its own Q, K, and V. However, each head transforms the embeddings into different representational subspaces, so while they all start with the same input, the projections they learn and the subsequent focus of their attention can differ significantly.\n",
        "\n",
        "### Corrected Process:\n",
        "\n",
        "1. **Input Embedding**: Every token in the sequence is first converted into a dense vector representation, which is the sum of its token embedding and its positional embedding.\n",
        "\n",
        "2. **Projection in Attention Heads**:\n",
        "   - Each attention head applies three distinct linear transformations to the entire embedding vector of each token, generating its own set of Q, K, and V:\n",
        "     - `Q = W_Q * embedding`\n",
        "     - `K = W_K * embedding`\n",
        "     - `V = W_V * embedding`\n",
        "   - These transformations are usually implemented as a single large linear layer (part of `self.c_attn` in your code) that outputs three times the embedding dimension, which is then split into Q, K, and V for each head.\n",
        "\n",
        "3. **Dimensionality**:\n",
        "   - If the model’s embedding size \\(C\\) is 768 and there are 12 heads, each head will output Q, K, and V vectors that are each \\(768/12 = 64\\) dimensions. Thus, each head transforms the full embedding vector into smaller, 64-dimensional Q, K, and V vectors.\n",
        "\n",
        "4. **Independent Attention Calculations**:\n",
        "   - Each head computes attention scores independently based on its Q, K, and V. The scores determine how much each element of the sequence (values) should be considered for each position in the output sequence.\n",
        "   - This allows the model to focus on different parts of the sequence differently in each head, capturing various types of relationships in the data (e.g., syntactic, semantic).\n",
        "\n",
        "### Visualization Example Revisited:\n",
        "\n",
        "- **Given**: A single token's embedding vector is `[e1, e2, ..., e768]`.\n",
        "- **In Each Head**: The entire vector is used but transformed differently:\n",
        "  - **Head 1** produces:\n",
        "    - `Q1 = W_Q1 * [e1, ..., e768]`\n",
        "    - `K1 = W_K1 * [e1, ..., e768]`\n",
        "    - `V1 = W_V1 * [e1, ..., e768]`\n",
        "  - **Head 2** through **Head 12** similarly use the entire vector but project it using different weights.\n",
        "\n",
        "By using the whole embedding vector in each head, the model can maintain a comprehensive understanding of each token while still allowing different heads to \"specialize\" in different types of relationships or features within the data. This design leverages the parallel processing power of GPUs effectively and is fundamental to the model's ability to generate nuanced and contextually appropriate outputs."
      ],
      "metadata": {
        "id": "2L4h_PQ7-MCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query: then what happens to the separate 12 heads in the end? do they just dot product each other?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The outputs from the 12 attention heads in a transformer model like GPT-2 don't simply undergo a dot product with each other. Instead, there's a more structured way of integrating these multiple streams of outputs to enhance the model's ability to capture diverse aspects of the information. Here's what happens step-by-step after each of the attention heads processes its input:\n",
        "\n",
        "### Combining the Outputs of Attention Heads\n",
        "\n",
        "1. **Independent Attention Computations**:\n",
        "   - Each head computes its own queries (Q), keys (K), and values (V).\n",
        "   - The attention mechanism for each head calculates a set of attention scores based on the dot product of queries and keys, which is then used to weight the values. This results in an output vector from each head that emphasizes different parts of the input sequence according to the head's learned attention pattern.\n",
        "\n",
        "2. **Concatenation of Head Outputs**:\n",
        "   - The output vectors from each head are concatenated back together. If each head produces a vector of dimension \\( \\frac{C}{N} \\) (where \\( C \\) is the embedding size and \\( N \\) is the number of heads), the concatenation of these vectors restores the original dimensionality \\( C \\). For example, if \\( C = 768 \\) and \\( N = 12 \\), each head outputs a vector of size 64, and concatenating these vectors results in a vector of size 768.\n",
        "\n",
        "3. **Final Linear Projection**:\n",
        "   - The concatenated vector is then passed through another learned linear projection (often represented by `self.c_proj` in your code). This step is crucial because it integrates the information from all the heads into a single output vector per token. This final projection layer combines the diverse representations produced by different heads into a coherent output that can be used in subsequent layers or for generating predictions.\n",
        "   - This projection not only serves to mix the information from different heads but also maps the concatenated output back to the desired embedding dimensionality, preparing it for the next layer or the output layer of the model.\n",
        "\n",
        "### Why This Process?\n",
        "\n",
        "This multi-headed approach allows the model to simultaneously focus on different types of relationships in the data—from very local (like specific syntactic ties) to more global (like thematic elements or long-range dependencies). By recombining the outputs, the model can leverage these diverse insights effectively, ensuring that no single aspect dominates unless it is contextually relevant.\n",
        "\n",
        "This design is one of the reasons transformers are particularly effective for tasks involving complex dependencies within data, such as natural language understanding and generation, where different parts of a sequence can have various types of influence on each other.\n",
        "\n",
        "### Visualization\n",
        "\n",
        "Imagine you have a sequence of three words processed by a transformer model with three attention heads. Each head processes the sequence and focuses on different relationships. The outputs from these heads might look something like this for each word:\n",
        "\n",
        "- **Head 1 Output**: [Focus on next word relationships]\n",
        "- **Head 2 Output**: [Focus on whole sentence theme]\n",
        "- **Head 3 Output**: [Focus on grammatical structure]\n",
        "\n",
        "These vectors are then concatenated and passed through a final projection, synthesizing a comprehensive representation that balances all these perspectives. This processed output is what feeds into subsequent layers or contributes to the final task-specific computations, such as predicting the next word in a sequence."
      ],
      "metadata": {
        "id": "DbW63s-c-MAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to coding"
      ],
      "metadata": {
        "id": "6R37AXxI-Ay0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0 # starts by checking that embedding size \"n_embd\" can be divisible by number of attention heads \"n_heads\". Crucial for equally\n",
        "                                                  # splitting the attention computations across the heads.\n",
        "\n",
        "                                                  # TIL, embeddings are splitted across number of heads. Each processes their own Q, K, V then all the dimensions once processed\n",
        "                                                  # are then concatenated back into a final vector.\n",
        "\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.LLMC_RESIDUAL_SCALE_FLAG = 1 # // CONFIRMED # treated as residual.\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        qkv = self.c_attn(x) # projects into qkv from input\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2) # splits it into 3 tensors\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        if FLASH:\n",
        "            # flashattention\n",
        "            y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "        else:\n",
        "            # manual implementation of attention\n",
        "            # this materializes the large (T,T) matrix for all the queries and keys\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # math.sqrt() used here instead of **-0.5 for the attention(q, k, v) equation\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "9cFUophd0omn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes"
      ],
      "metadata": {
        "id": "7gGd5IT7DlWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "query: ah yeah here we go it is. any components with \"LLMC_RESIDUAL_SCALE_FLAG = 1\" will be treated like a residual. does this changes anyt?\n",
        "\n",
        "---\n",
        "\n",
        "Thanks for sharing the weight initialization function. The inclusion of the `LLMC_RESIDUAL_SCALE_FLAG` in the initialization logic provides more context on how these specific layers are treated differently, especially in terms of initialization. Here's how this affects the overall behavior and processing in the model:\n",
        "\n",
        "### Understanding `LLMC_RESIDUAL_SCALE_FLAG`\n",
        "\n",
        "In your code, the `LLMC_RESIDUAL_SCALE_FLAG` is checked during the initialization of weights for layers within your model. Here’s how it impacts the initialization process:\n",
        "\n",
        "1. **Scaled Initialization**: If a layer (such as `self.c_proj` in your MLP module) is flagged with `LLMC_RESIDUAL_SCALE_FLAG`, the standard deviation for initializing weights is adjusted. Specifically, the standard deviation is set to `0.02 / math.sqrt(2 * self.config.n_layer)`, which scales down the variance of the initial weights. This scaled initialization helps in controlling the magnitude of outputs from these layers, which is particularly important for layers involved in residual connections. By scaling down the weights:\n",
        "   - **Enhances Training Stability**: Smaller weights in layers that are part of a residual pathway can help prevent exploding gradients, especially in deeper networks.\n",
        "   - **Facilitates Learning**: The scale factor rooted in the number of layers (`2 * self.config.n_layer`) suggests a deeper network requires more delicate handling of weight magnitudes to facilitate effective learning and information flow.\n",
        "\n",
        "2. **Purpose of Scaling**:\n",
        "   - In deep learning, especially with networks that use residual connections, it’s crucial to maintain a balance in the flow of gradients during backpropagation. Scaled initialization as applied here ensures that the updates to weights are not disproportionately large, which can destabilize learning, especially in the early phases.\n",
        "\n",
        "3. **Impact on Model Behavior**:\n",
        "   - This tailored initialization approach is crucial for ensuring that the contributions from the residual connections are effectively integrated without overwhelming the network’s learning process. It subtly tunes the network to leverage the benefits of deep architectures (more layers) while mitigating associated risks like vanishing or exploding gradients.\n",
        "\n",
        "### Broader Implications\n",
        "\n",
        "By using the `LLMC_RESIDUAL_SCALE_FLAG`, your model effectively acknowledges and addresses the unique challenges posed by residual connections in deep networks. This specialized treatment indicates a thoughtful approach to network design, where different components are initialized and scaled based on their role and impact on the overall architecture. It shows an advanced understanding of deep learning dynamics, specifically within the context of Transformers where depth and complexity can significantly influence performance.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Incorporating such flags and customized initialization schemes directly ties into overall network robustness and efficacy. It aligns well with best practices for training deep neural networks, ensuring that each component is optimized not just for function but also for harmonious integration into the larger system. This modification does indeed change how the model will learn and stabilize through training iterations, particularly enhancing its ability to scale to deep configurations effectively."
      ],
      "metadata": {
        "id": "XQqE9-VvDlUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "query: is MLP just a different name for feedforward? if not what does it do and what is it for\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Yes, within the context of neural network architectures, particularly in transformers, the term \"MLP\" (Multi-Layer Perceptron) is often used to refer to a specific component known as a feedforward neural network. However, it's important to clarify the traditional and specific uses of these terms:\n",
        "\n",
        "### Traditional Definitions:\n",
        "- **MLP (Multi-Layer Perceptron)**: Traditionally, an MLP is a type of neural network that consists of multiple layers of perceptrons (basic neural network units) with non-linear activation functions, except for the input nodes. It usually includes one or more hidden layers between the input and output layers. MLPs are capable of learning non-linear models and are used widely across various types of regression and classification problems.\n",
        "\n",
        "- **Feedforward Neural Network**: This is a broader category of neural networks where connections between the nodes do not form a cycle. This means the information moves in only one direction—forward—from the input nodes, through the hidden nodes (if any), and to the output nodes. There are no cycles or loops in the network. MLPs are a subset of feedforward neural networks.\n",
        "\n",
        "### In the Context of Transformers:\n",
        "- **MLP as Feedforward Network**: In transformer models, the term \"MLP\" is used specifically to describe the feedforward neural network component that follows the self-attention mechanism in each layer of the transformer. This MLP usually has two linear layers:\n",
        "  - The first linear layer expands the dimensionality of the input, often by a factor (such as 4 times the size of the model’s embedding dimension).\n",
        "  - This is followed by a non-linear activation function (like ReLU or GELU).\n",
        "  - The second linear layer then projects the output back to the original embedding dimension.\n",
        "\n",
        "### Role and Function of MLP in Transformers:\n",
        "- **Integration and Transformation**: The MLP component in transformers serves to further process each position of the input sequence independently of the others. After the self-attention layer aggregates information across the input sequence, taking into account the relationship between different positions, the MLP is used to integrate and transform this information at each position separately.\n",
        "  \n",
        "- **Complexity and Non-linearity**: The expansion and subsequent contraction of dimensions, combined with the non-linear activation, allow the MLP to capture complex patterns in the data. This is crucial for tasks that require understanding intricate relationships and dependencies, as often encountered in natural language processing.\n",
        "\n",
        "- **Enhancement of Model Capacity**: The inclusion of MLPs in each transformer layer significantly increases the model’s capacity (its ability to learn a wide variety of functions). This is essential for the performance on complex tasks that transformers are often employed for, such as machine translation, text generation, and various other forms of sequence-to-sequence modeling.\n",
        "\n",
        "### Summary:\n",
        "In summary, while the terms \"MLP\" and \"feedforward neural network\" can be used somewhat interchangeably in the context of transformers, an MLP in this setting specifically refers to a two-layer feedforward network with a non-linear activation used to process the output of the self-attention layers within each transformer block. This component plays a crucial role in enhancing the transformer's ability to process and integrate information across the sequence effectively."
      ],
      "metadata": {
        "id": "cK6-Z3qDFKjZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "query: oh wait a question about the c_proj which is a residual connection. so. in feed forward or MLP for example, if its a residual connection. how is its output now different? like as in, is the entire output of MLP now get residual treatment? or its partially treated as residually and mostly still affects the next layer etc\n",
        "\n",
        "---\n",
        "\n",
        "In the context of transformers and MLPs specifically, when we talk about residual connections, it typically means that the output of the MLP is added back to its original input (before the MLP processing), and then this combined output is often normalized (e.g., using layer normalization). This method is a key component in the architecture of transformers and has several important implications for how the network processes information and learns during training.\n",
        "\n",
        "### How Residual Connections Work in MLP within Transformers:\n",
        "\n",
        "1. **Processing in MLP**:\n",
        "   - The input \\( x \\) to the MLP is first transformed by an expansion layer (increases dimensionality), followed by a non-linear activation (like GELU), and finally a contraction layer (reduces dimensionality back to the original size).\n",
        "   - Let's denote the output of this process as \\( \\text{MLP}(x) \\).\n",
        "\n",
        "2. **Adding the Residual**:\n",
        "   - The output of the MLP \\( \\text{MLP}(x) \\) is then added to the original input \\( x \\). This operation is defined as:\n",
        "     \\[\n",
        "     y = x + \\text{MLP}(x)\n",
        "     \\]\n",
        "   - Here, \\( y \\) represents the residual output, which is the sum of the original input and the transformed output.\n",
        "\n",
        "3. **Layer Normalization**:\n",
        "   - After adding the residual, the result is usually normalized:\n",
        "     \\[\n",
        "     \\text{output} = \\text{LayerNorm}(y)\n",
        "     \\]\n",
        "   - This normalization helps stabilize the learning process by ensuring that the outputs of the layers have a mean of zero and a standard deviation of one, reducing the risk of vanishing or exploding gradients.\n",
        "\n",
        "### Impact of Residual Connections:\n",
        "\n",
        "- **Preservation of Information**: Residual connections help in preserving the information from the input throughout the layers of the network. By adding the input directly to the output, the network can maintain essential features that might be lost due to transformations within the MLP.\n",
        "\n",
        "- **Easing the Training of Deep Networks**: They allow gradients to flow directly through the network via the added pathways, facilitating the training of very deep networks by combating problems associated with vanishing gradients.\n",
        "\n",
        "- **Enhancing Learning Capability**: By combining the original information and the new features learned by the MLP, the network can refine its predictions, leading to potentially better performance and more nuanced understanding.\n",
        "\n",
        "### How the Output is Affected:\n",
        "\n",
        "- **Entire Output Treated Residually**: In transformers, typically the entire output of the MLP gets the \"residual treatment\". This means the whole transformed output is added back to the original input, not just a portion of it.\n",
        "\n",
        "- **Influence on Subsequent Layers**: The residual output, after normalization, is then passed to the next transformer layer (if any) or used in subsequent computations. This output now contains both the original and newly learned features, affecting how the subsequent layers process the input.\n",
        "\n",
        "### Summary:\n",
        "In summary, the residual connection in the MLP of a transformer model ensures that the entire output of the MLP is adjusted by its own input, enhancing the model's ability to leverage both new and existing information across different parts of the model. This approach is fundamental to the design of modern neural architectures like transformers, enabling them to achieve remarkable performance across various tasks."
      ],
      "metadata": {
        "id": "bAqwgl9AEiEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to Coding"
      ],
      "metadata": {
        "id": "pRh3popFDlPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module): # MLP is just another term for feed forward // its a subset of feed forward because unlike conventional feed forward\n",
        "                      # MLP actually expands to higher dimension before contract REFER TO NOTES ABOVE, which allows more complex relationships to be captured.\n",
        "                              # note this is just MLP for transformers.\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd) # increases dimension for more information?\n",
        "        self.gelu    = NewGELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.LLMC_RESIDUAL_SCALE_FLAG = 1 # treated as residual.\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x) # residual component\n",
        "        return x"
      ],
      "metadata": {
        "id": "oQZpOlwB0okr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Purpose and Functionality\n",
        "\n",
        "This setup is typical of transformer blocks, where each component layer (like self-attention and MLP) is wrapped with a residual connection followed by normalization. This architecture helps in several ways:\n",
        "\n",
        "- **Enhanced Gradient Flow**: Residual connections allow gradients to flow directly through the network during backpropagation, which can significantly improve training efficiency and enable training of very deep networks.\n",
        "\n",
        "- **Feature Preservation**: By adding the input directly to the output of processing units (attention and MLP), the network can preserve important features that might otherwise be lost or overly transformed.\n",
        "\n",
        "- **Capability Enrichment**: Layer normalization and subsequent processing ensure that the network can adaptively scale and shift the input features, refining the model's ability to handle a wide range of input distributions and feature scales.\n",
        "\n",
        "Overall, the `Block` class represents a fundamental building block of transformer models, encapsulating the essential mechanisms that make transformers powerful for handling sequential data across various tasks such as language modeling, text generation, and more."
      ],
      "metadata": {
        "id": "2T9cP-RlHClH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module): # this is implementaiton of single layer in a transformer.\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x)) # input x is first normalised by layer normalisation, then passed to self.attn for self-attention processing\n",
        "        x = x + self.mlp(self.ln_2(x)) # result of first residual connection is first normalised by self.ln_2. Then fed into mlp.\n",
        "        return x"
      ],
      "metadata": {
        "id": "nV2YK1UtAhZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# The main GPT-2 model\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768"
      ],
      "metadata": {
        "id": "dsSuhRNf0oix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__() # initialises base class nn.Module\n",
        "        self.config = config # stores configuration passed into the constructor\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict( # storing various components of GPT model. Each component can now be accessed using a key, just like a python dict.\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd), # maps token indices to token embeddings\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd), # provides positional information to tokens\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # list of Nx transformer block, refer to architecture.\n",
        "            ln_f = nn.LayerNorm(config.n_embd), # final layer normalisation applied to final output from blocks before finally outputting for transformer sequence processing\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # maps n_embd=768 dimension to vocab_size=50257 for generating predictions\n",
        "                                                                               # bias=False, no bias term here\n",
        "        self.lm_head.LLMC_SKIP_INIT = 1 # don't init this one, we will tie weights\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying <-- this is recommended based on research findings!\n",
        "        \"\"\"\n",
        "        This line ties the weights of the language modeling head to the token embedding weights, which helps reduce the model's parameter count and can improve\n",
        "        performance by reusing learned representations.\n",
        "        \"\"\"\n",
        "\n",
        "        # init all weights, use a torch rng object to be very careful\n",
        "        self.init_rng = torch.Generator() # initialises a RNG to ensure controlled randomness in weight initialisation\n",
        "        self.init_rng.manual_seed(42)\n",
        "        self.apply(self._init_weights) # applies _init_weights method to all submodules of the model, customising the weights' initialisation based on whether they are part\n",
        "                                       # of linear layers, embedding layers or other configurations (handling special flags)\n",
        "\n",
        "    def _init_weights(self, module): # this method is applied to all submodules of the model, as seen with the method above.\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "            std = 0.02 if not hasattr(module, 'LLMC_RESIDUAL_SCALE_FLAG') else 0.02/math.sqrt(2 * self.config.n_layer)\n",
        "                  # sets standard deviation for normal distribution used to initialise weights. If has RESIDUALflag, the std is reduced to \"0.02/math.sqrt(2 * self.config.n_layer)\"\n",
        "                        # This is recommended by GPT-2 paper, and this scaling helps control the varience of weights in deeper layers, aiding in stabilising training by\n",
        "                        # mitigating issues like exploding gradients.\n",
        "\n",
        "            # we want to skip initializing lm_head, which shares parameters with wte\n",
        "            # and wte was already initialized down below during the Embedding init\n",
        "            if not hasattr(module, 'LLMC_SKIP_INIT'):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=std, generator=self.init_rng) # skips intialisation if has LLMC_SKIP_INIT flag, else initialise using default values\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias) # if module has bias term, the bias is set to zero.\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02, generator=self.init_rng) # normal_ just initialises the weights of tensors to Gaussian distribution based on its params\n",
        "\n",
        "    def forward(self, idx, targets=None, return_logits=True):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        # there are performance reasons why not returning logits is prudent, if not needed\n",
        "        if not return_logits:\n",
        "            logits = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type, zero_stage):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print0(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print0(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        print0(f\"using fused AdamW: {use_fused}\")\n",
        "        if zero_stage == 1:\n",
        "            print0(\"using ZeroRedundancyOptimizer\")\n",
        "            optimizer = ZeroRedundancyOptimizer(**optim_groups[0], optimizer_class=torch.optim.AdamW,\n",
        "                                                lr=learning_rate, betas=betas, fused=use_fused)\n",
        "            optimizer.add_param_group(optim_groups[1])\n",
        "        else:\n",
        "            print0(\"using regular AdamW\")\n",
        "            optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, fused=use_fused)\n",
        "        return optimizer\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "vRa8YUyG0ogr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Our own simple Distributed Data Loader"
      ],
      "metadata": {
        "id": "cA2S7gyz0oeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _peek_data_shard(filename):\n",
        "    # only reads the header, returns header data\n",
        "    with open(filename, \"rb\") as f:\n",
        "        # first read the header, which is 256 int32 integers (4 bytes each)\n",
        "        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n",
        "    if header[0] != 20240520:\n",
        "        print(\"ERROR: magic number mismatch in the data .bin file!\")\n",
        "        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n",
        "        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n",
        "        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n",
        "        exit(1)\n",
        "    assert header[1] == 1, \"unsupported version\"\n",
        "    ntok = header[2] # number of tokens (claimed)\n",
        "    return ntok # for now just return the number of tokens"
      ],
      "metadata": {
        "id": "XfjKH4H31x9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _load_data_shard(filename):\n",
        "    with open(filename, \"rb\") as f:\n",
        "        # first read the header, which is 256 int32 integers (4 bytes each)\n",
        "        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n",
        "        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n",
        "        assert header[1] == 1, \"unsupported version\"\n",
        "        ntok = header[2] # number of tokens (claimed)\n",
        "        # the rest of it are tokens, stored as uint16\n",
        "        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n",
        "    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "Px9rBIWy1ydI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DistributedDataLoader:\n",
        "    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n",
        "        self.process_rank = process_rank\n",
        "        self.num_processes = num_processes\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "\n",
        "        # glob files that match the pattern\n",
        "        self.files = sorted(glob.glob(filename_pattern))\n",
        "        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n",
        "\n",
        "        # load and validate all data shards, count number of tokens in total\n",
        "        ntok_total = 0\n",
        "        for fname in self.files:\n",
        "            shard_ntok = _peek_data_shard(fname)\n",
        "            assert shard_ntok >= num_processes * B * T + 1\n",
        "            ntok_total += shard_ntok\n",
        "        self.ntok_total = ntok_total\n",
        "        print0(f\"DataLoader: total number of tokens: {ntok_total:,} across {len(self.files)} files\")\n",
        "\n",
        "        # kick things off\n",
        "        self.current_shard = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # we're being a bit clever here: if we already had shard 0 loaded,\n",
        "        # then don't do the work to reload it, just reset the pointer\n",
        "        if self.current_shard != 0:\n",
        "            self.current_shard = 0\n",
        "            self.tokens = _load_data_shard(self.files[self.current_shard])\n",
        "        self.current_position = self.process_rank * self.B * self.T\n",
        "\n",
        "    def advance(self): # advance to next data shard\n",
        "        self.current_shard = (self.current_shard + 1) % len(self.files)\n",
        "        self.current_position = self.process_rank * self.B * self.T\n",
        "        self.tokens = _load_data_shard(self.files[self.current_shard])\n",
        "\n",
        "    def next_batch(self):\n",
        "        B = self.B\n",
        "        T = self.T\n",
        "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
        "        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n",
        "        x = (buf[:-1]).view(B, T) # inputs\n",
        "        y = (buf[1:]).view(B, T) # targets\n",
        "        # advance the start pointer in current shard\n",
        "        self.current_position += B * T * self.num_processes\n",
        "        # if loading the next batch would be out of bounds advance the shard\n",
        "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
        "            self.advance()\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "7UuOqxFC11dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Python -> C bridge utilities for saving params/grads/activations to .bin files"
      ],
      "metadata": {
        "id": "Qbsl2_LI11-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_fp32(tensor, file):\n",
        "    t = tensor.detach().cpu().to(torch.float32)\n",
        "    b = t.numpy().tobytes()\n",
        "    file.write(b)"
      ],
      "metadata": {
        "id": "9v-pTAlE13RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_bf16(tensor, file):\n",
        "    t = tensor.detach().cpu().to(torch.bfloat16)\n",
        "    # numpy doesn't have bf16 datatype so we have to trick it\n",
        "    t = t.view(torch.int16) # trick: reinterpret as int16\n",
        "    b = t.numpy().tobytes()\n",
        "    file.write(b)"
      ],
      "metadata": {
        "id": "6Cpv_5RH14hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_tensors(model_tensors, L, file, dtype):\n",
        "    # writes the GPT-2 model's weights to a binary file\n",
        "    assert dtype in {\"float32\", \"bfloat16\"}\n",
        "    write_fun = write_fp32 if dtype == \"float32\" else write_bf16\n",
        "    write_fun(model_tensors[\"transformer.wte.weight\"], file) # (V, C)\n",
        "    write_fun(model_tensors[\"transformer.wpe.weight\"], file) # (T, C)\n",
        "    for i in range(L): # (L, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.ln_1.weight\"], file)\n",
        "    for i in range(L): # (L, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.ln_1.bias\"], file)\n",
        "    for i in range(L): # (L, 3C, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_attn.weight\"], file)\n",
        "    for i in range(L): # (L, 3C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_attn.bias\"], file)\n",
        "    for i in range(L): # (L, C, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_proj.weight\"], file)\n",
        "    for i in range(L): # (L, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_proj.bias\"], file)\n",
        "    for i in range(L): # (L, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.ln_2.weight\"], file)\n",
        "    for i in range(L): # (L, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.ln_2.bias\"], file)\n",
        "    for i in range(L): # (L, 4C, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_fc.weight\"], file)\n",
        "    for i in range(L): # (L, 4C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_fc.bias\"], file)\n",
        "    for i in range(L): # (L, C, 4C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_proj.weight\"], file)\n",
        "    for i in range(L): # (L, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_proj.bias\"], file)\n",
        "    write_fun(model_tensors[\"transformer.ln_f.weight\"], file) # (C, )\n",
        "    write_fun(model_tensors[\"transformer.ln_f.bias\"], file) # (C, )"
      ],
      "metadata": {
        "id": "kXT19UGU16lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def pad_vocab(tensor, multiple=128, value=0):\n",
        "    \"\"\"\n",
        "    The dimension of the vocab size in GPT-2 is 50,257\n",
        "    which is unfortunately a very unfriendly number for a lot of\n",
        "    matrix operations on the GPU. So we pad it to the nearest\n",
        "    friendlier multiple, e.g. 50,304 if multiple=128 when we\n",
        "    export the weights into C land. This is a NOOP algorithmically\n",
        "    and is only done to make the tensor operations more efficient.\n",
        "    \"\"\"\n",
        "    assert tensor.ndim == 2\n",
        "    V, C = tensor.shape\n",
        "    assert V == 50257, \"just being defensive here\"\n",
        "    # calculate padded vocab size by rounding up to nearest multiple\n",
        "    Vp = ((V + multiple - 1) // multiple) * multiple\n",
        "    # pad the tensor\n",
        "    pad_rows = Vp - V\n",
        "    padded = tensor if pad_rows == 0 else F.pad(tensor, (0, 0, 0, pad_rows), value=value)\n",
        "    assert padded.shape == (Vp, C)\n",
        "    return padded"
      ],
      "metadata": {
        "id": "P7Ia1zUt19TH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_model(model, filename, dtype):\n",
        "    # everything we need to instantiate the model\n",
        "    # 1) header is: version int, GPTConfig ints, padding to 1024 bytes\n",
        "    assert dtype in {\"float32\", \"bfloat16\"} # float16 todo maybe later\n",
        "    version = {\n",
        "        \"float32\": 3, # 3: all tensors are fp32, padded vocab\n",
        "        \"bfloat16\": 5, # 5: all tensors are bf16, padded vocab\n",
        "    }[dtype]\n",
        "    header = torch.zeros(256, dtype=torch.int32)\n",
        "    header[0] = 20240326 # magic\n",
        "    header[1] = version # checkpoint version\n",
        "    header[2] = model.config.block_size\n",
        "    header[3] = model.config.vocab_size\n",
        "    header[4] = model.config.n_layer\n",
        "    header[5] = model.config.n_head\n",
        "    header[6] = model.config.n_embd\n",
        "    # 2) the parameters follow the header\n",
        "    params = {name: param.cpu() for name, param in model.named_parameters()}\n",
        "    # pad the vocab to a multiple of 128 here at export, for efficiency in C\n",
        "    wte = params[\"transformer.wte.weight\"] # (V, C)\n",
        "    wte_padded = pad_vocab(wte) # (Vp, C)\n",
        "    params[\"transformer.wte.weight\"] = wte_padded # (Vp, C)\n",
        "    print(f\"padded vocab size from {wte.size(0)} to {wte_padded.size(0)}\")\n",
        "    header[7] = wte_padded.size(0) # padded vocab size store in header\n",
        "    # now write to file\n",
        "    with open(filename, \"wb\") as file:\n",
        "        file.write(header.numpy().tobytes()) # header\n",
        "        write_tensors(params, model.config.n_layer, file, dtype) # params\n",
        "    print(f\"wrote {filename}\")"
      ],
      "metadata": {
        "id": "lwI3hWWh1_UX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_state(model, x, y, logits, loss, filename):\n",
        "    # the state is used for debugging.\n",
        "    # it contains information about the input, logits, loss, and the parameter gradients\n",
        "    # this can be used for checking the computation correctness in C\n",
        "    header = torch.zeros(256, dtype=torch.int32)\n",
        "    header[0] = 20240327 # magic\n",
        "    header[1] = 2 # run state version = 2 (1 -> 2 for padded vocab changes)\n",
        "    header[2] = x.size(0) # batch size of the batch, B\n",
        "    header[3] = x.size(1) # temporal extent of the batch, T\n",
        "    grads = {name: param.grad.cpu() for name, param in model.named_parameters()}\n",
        "    # pad the vocab grads here as well, to mirror write_model\n",
        "    wte_grad = grads[\"transformer.wte.weight\"] # (V, C)\n",
        "    wte_grad_padded = pad_vocab(wte_grad, value=0) # (Vp, C) # TODO later maybe pad with nan?\n",
        "    grads[\"transformer.wte.weight\"] = wte_grad_padded # (Vp, C)\n",
        "    print(f\"padded vocab size in reference grads from {wte_grad.size(0)} to {wte_grad_padded.size(0)}\")\n",
        "    with open(filename, \"wb\") as file:\n",
        "        # header\n",
        "        file.write(header.numpy().tobytes())\n",
        "        # input x\n",
        "        file.write(x.cpu().numpy().astype(\"int32\").tobytes()) # (B, T)\n",
        "        # targets y\n",
        "        file.write(y.cpu().numpy().astype(\"int32\").tobytes()) # (B, T)\n",
        "        # logits (result of the model forward pass)\n",
        "        write_fp32(logits.cpu(), file)\n",
        "        # loss (single float, result of the cross entropy loss)\n",
        "        write_fp32(loss.cpu(), file)\n",
        "        # gradients\n",
        "        write_tensors(grads, model.config.n_layer, file, \"float32\")\n",
        "    print(f\"wrote {filename}\")"
      ],
      "metadata": {
        "id": "ZekQNsh22A4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_tokenizer(enc, filename):\n",
        "    n = enc.max_token_value + 1\n",
        "    header = torch.zeros(256, dtype=torch.int32)\n",
        "    header[0] = 20240328 # magic\n",
        "    header[1] = 2 # tokenizer version = 2 (1 -> 2: includes EOT token)\n",
        "    header[2] = n # number of tokens\n",
        "    header[3] = enc.eot_token # EOT token\n",
        "    with open(filename, \"wb\") as file:\n",
        "        file.write(header.numpy().tobytes())\n",
        "        for i in range(n):\n",
        "            b = enc.decode_bytes([i])\n",
        "            length = len(b)\n",
        "            assert length < 256, f\"Token length exceeds 255: {length}\"\n",
        "            file.write(struct.pack(\"<B\", length))  # Write the length as a 1-byte unsigned integer\n",
        "            file.write(b)  # Write the actual bytes\n",
        "    print(f\"wrote {filename}\")"
      ],
      "metadata": {
        "id": "7_bhpRxa2DIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# int main"
      ],
      "metadata": {
        "id": "CH26AhAU2Edl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print0(*args, **kwargs):\n",
        "    # modified print that only prints from the master process\n",
        "    # if this is not a distributed run, it's just a print\n",
        "    if int(os.environ.get(\"RANK\", 0)) == 0:\n",
        "        print(*args, **kwargs)"
      ],
      "metadata": {
        "id": "ZV7eT0Z12HXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import time\n",
        "    import argparse\n",
        "    import tiktoken\n",
        "    print0(f\"Running pytorch {torch.version.__version__}\")\n",
        "\n",
        "    # default settings will overfit a tiny batch of data\n",
        "    # and save model weights and debug state to disk on the first iteration\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # file system input / output\n",
        "    parser.add_argument(\"--input_bin\", type=str, default=\"dev/data/tinyshakespeare/tiny_shakespeare_val.bin\", help=\"input .bin to train on\")\n",
        "    parser.add_argument(\"--input_val_bin\", type=str, default=\"\", help=\"input .bin to eval validation loss on\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"\", help=\"output directory to which to write logs and checkpoints\")\n",
        "    parser.add_argument(\"--model\", type=str, default=\"gpt2\", help=\"gpt2|gpt2-medium|gpt2-large|gpt2-xl|d12|d24|d36|d48\")\n",
        "    # token layout for each step of the optimization\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=4, help=\"batch size, in units of #batch dimensions\")\n",
        "    parser.add_argument(\"--sequence_length\", type=int, default=64, help=\"sequence length\")\n",
        "    parser.add_argument(\"--total_batch_size\", type=int, default=256, help=\"total desired batch size, in units of #tokens\")\n",
        "    # workload (number of steps)\n",
        "    parser.add_argument(\"--num_iterations\", type=int, default=10, help=\"number of iterations to run\")\n",
        "    parser.add_argument(\"--inference_only\", type=int, default=0, help=\"only run inference\")\n",
        "    # optimization\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=1e-4, help=\"learning rate warmup iterations\")\n",
        "    parser.add_argument(\"--warmup_iters\", type=int, default=0, help=\"learning rate warmup iterations\")\n",
        "    parser.add_argument(\"--learning_rate_decay_frac\", type=float, default=1.0, help=\"learning rate warmup iterations\")\n",
        "    parser.add_argument(\"--weight_decay\", type=float, default=0.0, help=\"weight decay\")\n",
        "    parser.add_argument(\"--grad_clip\", type=float, default=1.0, help=\"maximum gradient magnitude\")\n",
        "    # evaluation\n",
        "    parser.add_argument(\"--val_loss_every\", type=int, default=0, help=\"every how mant steps to evaluate val loss?\")\n",
        "    parser.add_argument(\"--val_max_steps\", type=int, default=20, help=\"how many batches of val to average?\")\n",
        "    parser.add_argument(\"--sample_every\", type=int, default=0, help=\"how often to sample from the model?\")\n",
        "    # debugging\n",
        "    parser.add_argument(\"--overfit_single_batch\", type=int, default=1, help=\"overfit just one batch of data\")\n",
        "    # numerics\n",
        "    parser.add_argument(\"--tensorcores\", type=int, default=0, help=\"use tensorcores\")\n",
        "    # memory management\n",
        "    parser.add_argument(\"--device\", type=str, default=\"\", help=\"by default we autodetect, or set it here\")\n",
        "    parser.add_argument(\"--compile\", type=int, default=0, help=\"torch.compile the model\")\n",
        "    parser.add_argument(\"--flash\", type=int, default=0, help=\"use flash attention\")\n",
        "    parser.add_argument(\"--dtype\", type=str, default=\"float32\", help=\"float32|float16|bfloat16\")\n",
        "    parser.add_argument(\"--zero_stage\", type=int, default=0, help=\"zero redundancy optimizer stage (0/1/2/3)\")\n",
        "    # python -> C bridge\n",
        "    parser.add_argument(\"--write_tensors\", type=int, default=1, help=\"write tensors to disk\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # args error checking and convenience variables\n",
        "    B, T = args.batch_size, args.sequence_length\n",
        "    assert 1 <= T <= 1024\n",
        "    assert args.dtype in {\"float32\", \"float16\", \"bfloat16\"}\n",
        "    assert args.model in {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\", \"d12\", \"d24\", \"d36\", \"d48\"}\n",
        "\n",
        "    # set up DDP (distributed data parallel). torchrun sets this env variable\n",
        "    ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "    if ddp:\n",
        "        # use of DDP atm demands CUDA, we set the device appropriately according to rank\n",
        "        assert torch.cuda.is_available(), \"for now i think we need CUDA for DDP\"\n",
        "        init_process_group(backend='nccl')\n",
        "        ddp_rank = int(os.environ['RANK'])\n",
        "        ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "        ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "        device = f'cuda:{ddp_local_rank}'\n",
        "        torch.cuda.set_device(device)\n",
        "        master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "        seed_offset = 0 # each process gets the exact same seed\n",
        "        zero_stage = args.zero_stage\n",
        "    else:\n",
        "        ddp_rank = 0\n",
        "        ddp_local_rank = 0\n",
        "        zero_stage = 0\n",
        "        ddp_world_size = 1\n",
        "        master_process = True\n",
        "        seed_offset = 0\n",
        "        # select the device\n",
        "        if args.device:\n",
        "            # provided explicitly by the user\n",
        "            device = args.device\n",
        "        else:\n",
        "            # attempt to autodetect the device\n",
        "            device = \"cpu\"\n",
        "            if torch.cuda.is_available():\n",
        "                device = \"cuda\"\n",
        "            elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "                device = \"mps\"\n",
        "    print(f\"using device: {device}\")\n",
        "    device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "\n",
        "    # calculate gradient accumulation from the desired total batch size and the current run configuration\n",
        "    tokens_per_fwdbwd = B * T * ddp_world_size\n",
        "    assert args.total_batch_size % tokens_per_fwdbwd == 0\n",
        "    grad_accum_steps = args.total_batch_size // tokens_per_fwdbwd\n",
        "    print0(f\"total desired batch size: {args.total_batch_size}\")\n",
        "    print0(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
        "\n",
        "    # set up a context manager following the desired dtype and device\n",
        "    ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[args.dtype]\n",
        "    ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()\n",
        "\n",
        "    # rng / reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(42)\n",
        "\n",
        "    # set the torch precision mode to use TensorFloat32 (TF32) for matmuls\n",
        "    # docs https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html\n",
        "    if args.tensorcores:\n",
        "        torch.set_float32_matmul_precision('high')\n",
        "\n",
        "    # turn on/off flash attention\n",
        "    assert args.flash in {0, 1}\n",
        "    FLASH = args.flash\n",
        "\n",
        "    # init (and write) the tokenizer\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    if master_process and args.write_tensors: # tokenizer is technically not tensors but ok\n",
        "        write_tokenizer(enc, \"gpt2_tokenizer.bin\")\n",
        "\n",
        "    # init the model, either from scratch or from OpenAI pretrained checkpoint\n",
        "    if args.model[0] == \"d\":\n",
        "        # from scratch (random weights)\n",
        "        model_config = {\n",
        "            \"d12\": GPTConfig(block_size=1024, vocab_size=50257, n_layer=12, n_head=12, n_embd=768),\n",
        "            \"d24\": GPTConfig(block_size=1024, vocab_size=50257, n_layer=24, n_head=16, n_embd=1024),\n",
        "            \"d36\": GPTConfig(block_size=1024, vocab_size=50257, n_layer=36, n_head=20, n_embd=1280),\n",
        "            \"d48\": GPTConfig(block_size=1024, vocab_size=50257, n_layer=48, n_head=25, n_embd=1600),\n",
        "        }[args.model]\n",
        "        model = GPT(model_config)\n",
        "    else:\n",
        "        # load the GPT-2 model weights\n",
        "        model = GPT.from_pretrained(args.model)\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    if args.compile:\n",
        "        if hasattr(config, \"coordinate_descent_tuning\"):\n",
        "            config.coordinate_descent_tuning = True # suggested by @Chillee\n",
        "        print0(\"compiling the model...\")\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Our own version of a simple DistributedDataLoader\n",
        "\n",
        "    # load tokens\n",
        "    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n",
        "    val_loader = None\n",
        "    if args.input_val_bin:\n",
        "        val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # PyTorch -> C bridge: save some weights and state for C to load later as reference\n",
        "\n",
        "    # do one forward pass to generate ground truth for our C tests\n",
        "    if master_process and args.write_tensors and (not args.inference_only):\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits, loss = model(x, y)\n",
        "        loss.backward()\n",
        "        # save model params, in both float32 and bfloat16\n",
        "        model_to_size = {\"gpt2\": \"124M\", \"gpt2-medium\": \"355M\", \"gpt2-large\": \"774M\", \"gpt2-xl\": \"1558M\"}\n",
        "        model_to_size.update({f\"d{d}\": f\"d{d}\" for d in [12, 24, 36, 48]})\n",
        "        model_size_str = model_to_size[args.model] # e.g. \"124M\", or \"d12\"\n",
        "        write_model(model, f\"gpt2_{model_size_str}.bin\", dtype=\"float32\")\n",
        "        write_model(model, f\"gpt2_{model_size_str}_bf16.bin\", dtype=\"bfloat16\")\n",
        "        # save x, y, logits, loss, and parameter gradients, for debugging C\n",
        "        # always store these in fp32 to have an accurate reference (?)\n",
        "        write_state(model, x, y, logits, loss, f\"gpt2_{model_size_str}_debug_state.bin\")\n",
        "        # reset the train_loader for the optimization below\n",
        "        train_loader.reset()\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # main training loop\n",
        "\n",
        "    # here we wrap model into DDP container\n",
        "    if ddp:\n",
        "        model = DDP(model, device_ids=[ddp_local_rank])\n",
        "    raw_model = model.module if ddp else model # always contains the \"raw\" unwrapped model\n",
        "\n",
        "    # init the optimizer\n",
        "    optimizer = raw_model.configure_optimizers(weight_decay=args.weight_decay,\n",
        "                                               learning_rate=args.learning_rate, betas=(0.9, 0.95),\n",
        "                                               device_type=device, zero_stage=zero_stage)\n",
        "\n",
        "    # learning rate decay scheduler (cosine with warmup)\n",
        "    def get_lr(it):\n",
        "        min_lr = args.learning_rate * args.learning_rate_decay_frac\n",
        "        # 1) linear warmup for warmup_iters steps\n",
        "        if it < args.warmup_iters:\n",
        "            return args.learning_rate * (it+1) / args.warmup_iters\n",
        "        # 2) if it > lr_decay_iters, return min learning rate\n",
        "        if it > args.num_iterations:\n",
        "            return min_lr\n",
        "        # 3) in between, use cosine decay down to min learning rate\n",
        "        decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n",
        "        assert 0 <= decay_ratio <= 1\n",
        "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
        "        return min_lr + coeff * (args.learning_rate - min_lr)\n",
        "\n",
        "    # create the logging directory if it does not exist\n",
        "    logfile = None\n",
        "    if args.output_dir:\n",
        "        os.makedirs(args.output_dir, exist_ok=True)\n",
        "        logfile = os.path.join(args.output_dir, \"main.log\")\n",
        "        # create the log file \"main.log\" inside it, and wipe it clean\n",
        "        with open(logfile, \"w\") as f:\n",
        "            pass\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "    timings = []\n",
        "    norm = -1.0   # dummy value to print in inference-only mode\n",
        "    for step in range(args.num_iterations + 1):\n",
        "        t0 = time.time()\n",
        "        last_step = (step == args.num_iterations)\n",
        "\n",
        "        # once in a while evaluate the validation dataset\n",
        "        if (args.val_loss_every > 0 \\\n",
        "            and (step % args.val_loss_every == 0 or last_step)) \\\n",
        "            and (val_loader is not None):\n",
        "            model.eval()\n",
        "            val_loader.reset()\n",
        "            with torch.no_grad():\n",
        "                val_loss = 0.0\n",
        "                for _ in range(args.val_max_steps):\n",
        "                    x, y = val_loader.next_batch()\n",
        "                    x, y = x.to(device), y.to(device)\n",
        "                    _, loss = model(x, y, return_logits=False)\n",
        "                    val_loss += loss.item()\n",
        "                val_loss /= args.val_max_steps\n",
        "            # log to console and to file\n",
        "            print0(f\"val loss {val_loss}\")\n",
        "            if master_process and logfile is not None:\n",
        "                with open(logfile, \"a\") as f:\n",
        "                    f.write(\"s:%d tel:%f\\n\" % (step, val_loss))\n",
        "\n",
        "        # once in a while perform model inference on the master process\n",
        "        if (args.sample_every > 0 \\\n",
        "            and (step % args.sample_every == 0 or last_step)) \\\n",
        "            and master_process:\n",
        "            model.eval()\n",
        "            # before we end, let's also do one round of inference\n",
        "            # we'll kick off the generation with \"<|endoftext|>\", which designates the start of a new sequence\n",
        "            start_ids = [enc.eot_token]\n",
        "            xg = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "            max_new_tokens = 32\n",
        "            temperature = 1.0\n",
        "            top_k = 40\n",
        "            yg = raw_model.generate(xg, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print0('---------------')\n",
        "            print0(enc.decode(yg[0].tolist()))\n",
        "            print0('---------------')\n",
        "\n",
        "        # bit confusing: we want to make sure to eval and sample on 0th iteration\n",
        "        # but also after the very last iteration. so we loop for step <= num_iterations\n",
        "        # instead of just < num_iterations (one extra due to <=), only to do\n",
        "        # the validation/sampling one last time, and then we break right here as we're done.\n",
        "        if last_step:\n",
        "            break\n",
        "\n",
        "        # --------------- TRAINING SECTION BEGIN -----------------\n",
        "        model.train()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        # if we are trying to overfit a single batch, we reset the loader here\n",
        "        if args.overfit_single_batch:\n",
        "            train_loader.reset()\n",
        "        # micro-batch loop where we do gradient accumulation to reach desired total batch size\n",
        "        lossf = 0.0 # for getting the mean loss (as simple float) over the accumulation steps\n",
        "        for micro_step in range(grad_accum_steps):\n",
        "            # fetch a batch\n",
        "            x, y = train_loader.next_batch()\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            if ddp:\n",
        "                # we want only the last micro-step to sync grads in a DDP model\n",
        "                # the official way to do this is with model.no_sync(), but that is a\n",
        "                # context manager that bloats the code, so we just toggle this variable\n",
        "                model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n",
        "            # forward pass\n",
        "            with ctx:\n",
        "                _, loss = model(x, y, return_logits=False)\n",
        "                # we have to scale the loss to account for gradient accumulation,\n",
        "                # because the gradients just add on each successive backward().\n",
        "                # addition of gradients corresponds to a SUM in the objective, but\n",
        "                # instead of a SUM we want MEAN, so we scale the loss here\n",
        "                loss = loss / grad_accum_steps\n",
        "                lossf += loss.detach() # keep track of the mean loss\n",
        "            # backward pass\n",
        "            if not args.inference_only:\n",
        "                loss.backward()\n",
        "        if ddp:\n",
        "            dist.all_reduce(lossf, op=dist.ReduceOp.AVG)\n",
        "        lossf = lossf.item()\n",
        "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
        "        # determine and set the learning rate for this iteration\n",
        "        lr = get_lr(step)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        # step the optimizer\n",
        "        optimizer.step()\n",
        "        # --------------- TRAINING SECTION END -------------------\n",
        "        # everything that follows now is just diagnostics, prints, logging, etc.\n",
        "\n",
        "        # wait on the CPU for all device work to end so we get accurate per-iteration timings below\n",
        "        if device == \"mps\":\n",
        "            torch.mps.synchronize()\n",
        "        elif device == \"cuda\":\n",
        "            torch.cuda.synchronize()\n",
        "        # time and print\n",
        "        t1 = time.time()\n",
        "        # the 0th iteration is often an outlier (much slower) => skip logging it\n",
        "        tokens_per_second = grad_accum_steps * ddp_world_size * B * T / (t1-t0)\n",
        "        print0(f\"step {step+1:4d}/{args.num_iterations} | train loss {lossf:.6f} | norm {norm:.4f} | lr {lr:.2e} | ({(t1-t0)*1000:.2f} ms | {tokens_per_second:.0f} tok/s)\")\n",
        "        # log to logile\n",
        "        if master_process and logfile is not None:\n",
        "            with open(logfile, \"a\") as f:\n",
        "                f.write(\"s:%d trl:%f\\n\" % (step, lossf))\n",
        "\n",
        "        # keep track of smooth timings, last 20 iterations\n",
        "        if step > 0 and step > args.num_iterations - 20:\n",
        "            timings.append(t1-t0)\n",
        "\n",
        "    # print the average of the last 20 timings, to get something smooth-ish\n",
        "    timings = timings[-20:]\n",
        "    print0(f\"final {len(timings)} iters avg: {np.mean(timings)*1000:.3f}ms\")\n",
        "    print0(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # clean up nice\n",
        "    if ddp:\n",
        "        destroy_process_group()"
      ],
      "metadata": {
        "id": "WERtaSpT2HtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Below is Torch's official implementation\n",
        "Module is from ```from .module import Module```"
      ],
      "metadata": {
        "id": "Eg1UjxJK5Sb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(Module):\n",
        "    r\"\"\"Applies Layer Normalization over a mini-batch of inputs.\n",
        "\n",
        "    This layer implements the operation as described in\n",
        "    the paper `Layer Normalization <https://arxiv.org/abs/1607.06450>`__\n",
        "\n",
        "    .. math::\n",
        "        y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n",
        "\n",
        "    The mean and standard-deviation are calculated over the last `D` dimensions, where `D`\n",
        "    is the dimension of :attr:`normalized_shape`. For example, if :attr:`normalized_shape`\n",
        "    is ``(3, 5)`` (a 2-dimensional shape), the mean and standard-deviation are computed over\n",
        "    the last 2 dimensions of the input (i.e. ``input.mean((-2, -1))``).\n",
        "    :math:`\\gamma` and :math:`\\beta` are learnable affine transform parameters of\n",
        "    :attr:`normalized_shape` if :attr:`elementwise_affine` is ``True``.\n",
        "    The standard-deviation is calculated via the biased estimator, equivalent to\n",
        "    `torch.var(input, unbiased=False)`.\n",
        "\n",
        "    .. note::\n",
        "        Unlike Batch Normalization and Instance Normalization, which applies\n",
        "        scalar scale and bias for each entire channel/plane with the\n",
        "        :attr:`affine` option, Layer Normalization applies per-element scale and\n",
        "        bias with :attr:`elementwise_affine`.\n",
        "\n",
        "    This layer uses statistics computed from input data in both training and\n",
        "    evaluation modes.\n",
        "\n",
        "    Args:\n",
        "        normalized_shape (int or list or torch.Size): input shape from an expected input\n",
        "            of size\n",
        "\n",
        "            .. math::\n",
        "                [* \\times \\text{normalized\\_shape}[0] \\times \\text{normalized\\_shape}[1]\n",
        "                    \\times \\ldots \\times \\text{normalized\\_shape}[-1]]\n",
        "\n",
        "            If a single integer is used, it is treated as a singleton list, and this module will\n",
        "            normalize over the last dimension which is expected to be of that specific size.\n",
        "        eps: a value added to the denominator for numerical stability. Default: 1e-5\n",
        "        elementwise_affine: a boolean value that when set to ``True``, this module\n",
        "            has learnable per-element affine parameters initialized to ones (for weights)\n",
        "            and zeros (for biases). Default: ``True``.\n",
        "        bias: If set to ``False``, the layer will not learn an additive bias (only relevant if\n",
        "            :attr:`elementwise_affine` is ``True``). Default: ``True``.\n",
        "\n",
        "    Attributes:\n",
        "        weight: the learnable weights of the module of shape\n",
        "            :math:`\\text{normalized\\_shape}` when :attr:`elementwise_affine` is set to ``True``.\n",
        "            The values are initialized to 1.\n",
        "        bias:   the learnable bias of the module of shape\n",
        "                :math:`\\text{normalized\\_shape}` when :attr:`elementwise_affine` is set to ``True``.\n",
        "                The values are initialized to 0.\n",
        "\n",
        "    Shape:\n",
        "        - Input: :math:`(N, *)`\n",
        "        - Output: :math:`(N, *)` (same shape as input)\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        >>> # NLP Example\n",
        "        >>> batch, sentence_length, embedding_dim = 20, 5, 10\n",
        "        >>> embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
        "        >>> layer_norm = nn.LayerNorm(embedding_dim)\n",
        "        >>> # Activate module\n",
        "        >>> layer_norm(embedding)\n",
        "        >>>\n",
        "        >>> # Image Example\n",
        "        >>> N, C, H, W = 20, 5, 10, 10\n",
        "        >>> input = torch.randn(N, C, H, W)\n",
        "        >>> # Normalize over the last three dimensions (i.e. the channel and spatial dimensions)\n",
        "        >>> # as shown in the image below\n",
        "        >>> layer_norm = nn.LayerNorm([C, H, W])\n",
        "        >>> output = layer_norm(input)\n",
        "\n",
        "    .. image:: ../_static/img/nn/layer_norm.jpg\n",
        "        :scale: 50 %\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    __constants__ = ['normalized_shape', 'eps', 'elementwise_affine']\n",
        "    normalized_shape: Tuple[int, ...]\n",
        "    eps: float\n",
        "    elementwise_affine: bool\n",
        "\n",
        "    def __init__(self, normalized_shape: _shape_t, eps: float = 1e-5, elementwise_affine: bool = True,\n",
        "                 bias: bool = True, device=None, dtype=None) -> None:\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super().__init__()\n",
        "        if isinstance(normalized_shape, numbers.Integral):\n",
        "            # mypy error: incompatible types in assignment\n",
        "            normalized_shape = (normalized_shape,)  # type: ignore[assignment]\n",
        "        self.normalized_shape = tuple(normalized_shape)  # type: ignore[arg-type]\n",
        "        self.eps = eps\n",
        "        self.elementwise_affine = elementwise_affine\n",
        "        if self.elementwise_affine:\n",
        "            self.weight = Parameter(torch.empty(self.normalized_shape, **factory_kwargs))\n",
        "            if bias:\n",
        "                self.bias = Parameter(torch.empty(self.normalized_shape, **factory_kwargs))\n",
        "            else:\n",
        "                self.register_parameter('bias', None)\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        if self.elementwise_affine:\n",
        "            init.ones_(self.weight)\n",
        "            if self.bias is not None:\n",
        "                init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        return F.layer_norm(input, self.normalized_shape, self.weight, self.bias, self.eps)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return '{normalized_shape}, eps={eps}, ' \\\n",
        "            'elementwise_affine={elementwise_affine}'.format(**self.__dict__)"
      ],
      "metadata": {
        "id": "0o6oOvpp5ayS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the raw implementation of torch.nn.Functional's ```F.layer_norm```"
      ],
      "metadata": {
        "id": "NjlewLCS6fKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_norm(input: Tensor, normalized_shape: List[int], weight: Optional[Tensor] = None, bias: Optional[Tensor] = None, eps: float = 1e-5) -> Tensor:\n",
        "    r\"\"\"Apply Layer Normalization for last certain number of dimensions.\n",
        "\n",
        "    See :class:`~torch.nn.LayerNorm` for details.\n",
        "    \"\"\"\n",
        "    if has_torch_function_variadic(input, weight, bias):\n",
        "        return handle_torch_function(layer_norm, (input, weight, bias), input, normalized_shape, weight=weight, bias=bias, eps=eps)\n",
        "\n",
        "    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)"
      ],
      "metadata": {
        "id": "dBXVmFur6cgx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}