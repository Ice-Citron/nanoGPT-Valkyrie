{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install os==2.1.4\n",
        "!pip install numpy==2.0.1\n",
        "\n",
        "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0\n",
        "\n",
        "!pip install dataclasses==0.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_d99aNx0oK4",
        "outputId": "805d88bc-1074-48af-85a2-30155bd11d6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement os==2.1.4 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for os==2.1.4\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting numpy==2.0.1\n",
            "  Downloading numpy-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xgboost 2.1.0 requires nvidia-nccl-cu12; platform_system == \"Linux\" and platform_machine != \"aarch64\", which is not installed.\n",
            "accelerate 0.32.1 requires numpy<2.0.0,>=1.17, but you have numpy 2.0.1 which is incompatible.\n",
            "albucore 0.0.12 requires numpy<2,>=1.24.4, but you have numpy 2.0.1 which is incompatible.\n",
            "arviz 0.18.0 requires numpy<2.0,>=1.23.0, but you have numpy 2.0.1 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.1 which is incompatible.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.1 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.1 which is incompatible.\n",
            "ibis-framework 8.0.0 requires numpy<2,>=1, but you have numpy 2.0.1 which is incompatible.\n",
            "pandas 2.1.4 requires numpy<2,>=1.22.4; python_version < \"3.11\", but you have numpy 2.0.1 which is incompatible.\n",
            "rmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.1 which is incompatible.\n",
            "scikit-learn 1.3.2 requires numpy<2.0,>=1.17.3, but you have numpy 2.0.1 which is incompatible.\n",
            "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.0.1 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.0.1 which is incompatible.\n",
            "transformers 4.42.4 requires numpy<2.0,>=1.17, but you have numpy 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.0.1\n",
            "Collecting torch==2.3.0\n",
            "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision==0.18.0\n",
            "  Downloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torchaudio==2.3.0\n",
            "  Downloading torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.3.0 (from torch==2.3.0)\n",
            "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.0) (2.0.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.0) (9.4.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r8gjIVSh-z7"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Reference code for GPT-2 training and inference.\n",
        "Will save the model weights into files, to be read from C as initialization.\n",
        "\n",
        "References:\n",
        "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
        "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
        "2) huggingface/transformers PyTorch implementation:\n",
        "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
        "\n",
        "Example launches to only benchmark the speed of bfloat16 compiled GPU training:\n",
        "1 GPU:\n",
        "python train_gpt2.py --write_tensors=0 --num_iterations=50 --sequence_length=1024 --compile=1 --tensorcores=1 --dtype=bfloat16\n",
        "you can also turn on flash-attention by appending --flash=1\n",
        "4 GPU:\n",
        "torchrun --standalone --nproc_per_node=4 train_gpt2.py --write_tensors=0 --num_iterations=50 --sequence_length=1024 --compile=1 --tensorcores=1 --dtype=bfloat16\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import glob\n",
        "import struct\n",
        "import inspect\n",
        "from contextlib import nullcontext\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch._inductor.config as config\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "from torch.distributed.optim import ZeroRedundancyOptimizer\n",
        "import torch.distributed as dist"
      ],
      "metadata": {
        "id": "wCn2Wi310osP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# PyTorch nn.Module definitions for the GPT-2 model"
      ],
      "metadata": {
        "id": "fM6pPFCw1lFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NewGELU(nn.Module):\n",
        "    \"\"\"Careful there are a few versions of GeLU, this one is the exact one used by OpenAI\"\"\"\n",
        "    def forward(self, input):\n",
        "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))"
      ],
      "metadata": {
        "id": "4lJcBc4b0oqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using a global to toggle flash-attention\n",
        "FLASH = 0"
      ],
      "metadata": {
        "id": "fzZ84Pbc0oov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.LLMC_RESIDUAL_SCALE_FLAG = 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        if FLASH:\n",
        "            # flashattention\n",
        "            y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "        else:\n",
        "            # manual implementation of attention\n",
        "            # this materializes the large (T,T) matrix for all the queries and keys\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "9cFUophd0omn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = NewGELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.LLMC_RESIDUAL_SCALE_FLAG = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "oQZpOlwB0okr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# The main GPT-2 model\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768"
      ],
      "metadata": {
        "id": "dsSuhRNf0oix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.lm_head.LLMC_SKIP_INIT = 1 # don't init this one, we will tie weights\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights, use a torch rng object to be very careful\n",
        "        self.init_rng = torch.Generator()\n",
        "        self.init_rng.manual_seed(42)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "            std = 0.02 if not hasattr(module, 'LLMC_RESIDUAL_SCALE_FLAG') else 0.02/math.sqrt(2 * self.config.n_layer)\n",
        "            # we want to skip initializing lm_head, which shares parameters with wte\n",
        "            # and wte was already initialized down below during the Embedding init\n",
        "            if not hasattr(module, 'LLMC_SKIP_INIT'):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=std, generator=self.init_rng)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02, generator=self.init_rng)\n",
        "\n",
        "    def forward(self, idx, targets=None, return_logits=True):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        # there are performance reasons why not returning logits is prudent, if not needed\n",
        "        if not return_logits:\n",
        "            logits = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type, zero_stage):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print0(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print0(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        print0(f\"using fused AdamW: {use_fused}\")\n",
        "        if zero_stage == 1:\n",
        "            print0(\"using ZeroRedundancyOptimizer\")\n",
        "            optimizer = ZeroRedundancyOptimizer(**optim_groups[0], optimizer_class=torch.optim.AdamW,\n",
        "                                                lr=learning_rate, betas=betas, fused=use_fused)\n",
        "            optimizer.add_param_group(optim_groups[1])\n",
        "        else:\n",
        "            print0(\"using regular AdamW\")\n",
        "            optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, fused=use_fused)\n",
        "        return optimizer\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "vRa8YUyG0ogr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Our own simple Distributed Data Loader"
      ],
      "metadata": {
        "id": "cA2S7gyz0oeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _peek_data_shard(filename):\n",
        "    # only reads the header, returns header data\n",
        "    with open(filename, \"rb\") as f:\n",
        "        # first read the header, which is 256 int32 integers (4 bytes each)\n",
        "        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n",
        "    if header[0] != 20240520:\n",
        "        print(\"ERROR: magic number mismatch in the data .bin file!\")\n",
        "        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n",
        "        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n",
        "        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n",
        "        exit(1)\n",
        "    assert header[1] == 1, \"unsupported version\"\n",
        "    ntok = header[2] # number of tokens (claimed)\n",
        "    return ntok # for now just return the number of tokens"
      ],
      "metadata": {
        "id": "XfjKH4H31x9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _load_data_shard(filename):\n",
        "    with open(filename, \"rb\") as f:\n",
        "        # first read the header, which is 256 int32 integers (4 bytes each)\n",
        "        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n",
        "        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n",
        "        assert header[1] == 1, \"unsupported version\"\n",
        "        ntok = header[2] # number of tokens (claimed)\n",
        "        # the rest of it are tokens, stored as uint16\n",
        "        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n",
        "    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "Px9rBIWy1ydI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DistributedDataLoader:\n",
        "    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n",
        "        self.process_rank = process_rank\n",
        "        self.num_processes = num_processes\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "\n",
        "        # glob files that match the pattern\n",
        "        self.files = sorted(glob.glob(filename_pattern))\n",
        "        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n",
        "\n",
        "        # load and validate all data shards, count number of tokens in total\n",
        "        ntok_total = 0\n",
        "        for fname in self.files:\n",
        "            shard_ntok = _peek_data_shard(fname)\n",
        "            assert shard_ntok >= num_processes * B * T + 1\n",
        "            ntok_total += shard_ntok\n",
        "        self.ntok_total = ntok_total\n",
        "        print0(f\"DataLoader: total number of tokens: {ntok_total:,} across {len(self.files)} files\")\n",
        "\n",
        "        # kick things off\n",
        "        self.current_shard = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # we're being a bit clever here: if we already had shard 0 loaded,\n",
        "        # then don't do the work to reload it, just reset the pointer\n",
        "        if self.current_shard != 0:\n",
        "            self.current_shard = 0\n",
        "            self.tokens = _load_data_shard(self.files[self.current_shard])\n",
        "        self.current_position = self.process_rank * self.B * self.T\n",
        "\n",
        "    def advance(self): # advance to next data shard\n",
        "        self.current_shard = (self.current_shard + 1) % len(self.files)\n",
        "        self.current_position = self.process_rank * self.B * self.T\n",
        "        self.tokens = _load_data_shard(self.files[self.current_shard])\n",
        "\n",
        "    def next_batch(self):\n",
        "        B = self.B\n",
        "        T = self.T\n",
        "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
        "        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n",
        "        x = (buf[:-1]).view(B, T) # inputs\n",
        "        y = (buf[1:]).view(B, T) # targets\n",
        "        # advance the start pointer in current shard\n",
        "        self.current_position += B * T * self.num_processes\n",
        "        # if loading the next batch would be out of bounds advance the shard\n",
        "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
        "            self.advance()\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "7UuOqxFC11dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Python -> C bridge utilities for saving params/grads/activations to .bin files"
      ],
      "metadata": {
        "id": "Qbsl2_LI11-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_fp32(tensor, file):\n",
        "    t = tensor.detach().cpu().to(torch.float32)\n",
        "    b = t.numpy().tobytes()\n",
        "    file.write(b)"
      ],
      "metadata": {
        "id": "9v-pTAlE13RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_bf16(tensor, file):\n",
        "    t = tensor.detach().cpu().to(torch.bfloat16)\n",
        "    # numpy doesn't have bf16 datatype so we have to trick it\n",
        "    t = t.view(torch.int16) # trick: reinterpret as int16\n",
        "    b = t.numpy().tobytes()\n",
        "    file.write(b)"
      ],
      "metadata": {
        "id": "6Cpv_5RH14hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_tensors(model_tensors, L, file, dtype):\n",
        "    # writes the GPT-2 model's weights to a binary file\n",
        "    assert dtype in {\"float32\", \"bfloat16\"}\n",
        "    write_fun = write_fp32 if dtype == \"float32\" else write_bf16\n",
        "    write_fun(model_tensors[\"transformer.wte.weight\"], file) # (V, C)\n",
        "    write_fun(model_tensors[\"transformer.wpe.weight\"], file) # (T, C)\n",
        "    for i in range(L): # (L, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.ln_1.weight\"], file)\n",
        "    for i in range(L): # (L, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.ln_1.bias\"], file)\n",
        "    for i in range(L): # (L, 3C, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_attn.weight\"], file)\n",
        "    for i in range(L): # (L, 3C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_attn.bias\"], file)\n",
        "    for i in range(L): # (L, C, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_proj.weight\"], file)\n",
        "    for i in range(L): # (L, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_proj.bias\"], file)\n",
        "    for i in range(L): # (L, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.ln_2.weight\"], file)\n",
        "    for i in range(L): # (L, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.ln_2.bias\"], file)\n",
        "    for i in range(L): # (L, 4C, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_fc.weight\"], file)\n",
        "    for i in range(L): # (L, 4C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_fc.bias\"], file)\n",
        "    for i in range(L): # (L, C, 4C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_proj.weight\"], file)\n",
        "    for i in range(L): # (L, C)\n",
        "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_proj.bias\"], file)\n",
        "    write_fun(model_tensors[\"transformer.ln_f.weight\"], file) # (C, )\n",
        "    write_fun(model_tensors[\"transformer.ln_f.bias\"], file) # (C, )"
      ],
      "metadata": {
        "id": "kXT19UGU16lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def pad_vocab(tensor, multiple=128, value=0):\n",
        "    \"\"\"\n",
        "    The dimension of the vocab size in GPT-2 is 50,257\n",
        "    which is unfortunately a very unfriendly number for a lot of\n",
        "    matrix operations on the GPU. So we pad it to the nearest\n",
        "    friendlier multiple, e.g. 50,304 if multiple=128 when we\n",
        "    export the weights into C land. This is a NOOP algorithmically\n",
        "    and is only done to make the tensor operations more efficient.\n",
        "    \"\"\"\n",
        "    assert tensor.ndim == 2\n",
        "    V, C = tensor.shape\n",
        "    assert V == 50257, \"just being defensive here\"\n",
        "    # calculate padded vocab size by rounding up to nearest multiple\n",
        "    Vp = ((V + multiple - 1) // multiple) * multiple\n",
        "    # pad the tensor\n",
        "    pad_rows = Vp - V\n",
        "    padded = tensor if pad_rows == 0 else F.pad(tensor, (0, 0, 0, pad_rows), value=value)\n",
        "    assert padded.shape == (Vp, C)\n",
        "    return padded"
      ],
      "metadata": {
        "id": "P7Ia1zUt19TH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_model(model, filename, dtype):\n",
        "    # everything we need to instantiate the model\n",
        "    # 1) header is: version int, GPTConfig ints, padding to 1024 bytes\n",
        "    assert dtype in {\"float32\", \"bfloat16\"} # float16 todo maybe later\n",
        "    version = {\n",
        "        \"float32\": 3, # 3: all tensors are fp32, padded vocab\n",
        "        \"bfloat16\": 5, # 5: all tensors are bf16, padded vocab\n",
        "    }[dtype]\n",
        "    header = torch.zeros(256, dtype=torch.int32)\n",
        "    header[0] = 20240326 # magic\n",
        "    header[1] = version # checkpoint version\n",
        "    header[2] = model.config.block_size\n",
        "    header[3] = model.config.vocab_size\n",
        "    header[4] = model.config.n_layer\n",
        "    header[5] = model.config.n_head\n",
        "    header[6] = model.config.n_embd\n",
        "    # 2) the parameters follow the header\n",
        "    params = {name: param.cpu() for name, param in model.named_parameters()}\n",
        "    # pad the vocab to a multiple of 128 here at export, for efficiency in C\n",
        "    wte = params[\"transformer.wte.weight\"] # (V, C)\n",
        "    wte_padded = pad_vocab(wte) # (Vp, C)\n",
        "    params[\"transformer.wte.weight\"] = wte_padded # (Vp, C)\n",
        "    print(f\"padded vocab size from {wte.size(0)} to {wte_padded.size(0)}\")\n",
        "    header[7] = wte_padded.size(0) # padded vocab size store in header\n",
        "    # now write to file\n",
        "    with open(filename, \"wb\") as file:\n",
        "        file.write(header.numpy().tobytes()) # header\n",
        "        write_tensors(params, model.config.n_layer, file, dtype) # params\n",
        "    print(f\"wrote {filename}\")"
      ],
      "metadata": {
        "id": "lwI3hWWh1_UX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_state(model, x, y, logits, loss, filename):\n",
        "    # the state is used for debugging.\n",
        "    # it contains information about the input, logits, loss, and the parameter gradients\n",
        "    # this can be used for checking the computation correctness in C\n",
        "    header = torch.zeros(256, dtype=torch.int32)\n",
        "    header[0] = 20240327 # magic\n",
        "    header[1] = 2 # run state version = 2 (1 -> 2 for padded vocab changes)\n",
        "    header[2] = x.size(0) # batch size of the batch, B\n",
        "    header[3] = x.size(1) # temporal extent of the batch, T\n",
        "    grads = {name: param.grad.cpu() for name, param in model.named_parameters()}\n",
        "    # pad the vocab grads here as well, to mirror write_model\n",
        "    wte_grad = grads[\"transformer.wte.weight\"] # (V, C)\n",
        "    wte_grad_padded = pad_vocab(wte_grad, value=0) # (Vp, C) # TODO later maybe pad with nan?\n",
        "    grads[\"transformer.wte.weight\"] = wte_grad_padded # (Vp, C)\n",
        "    print(f\"padded vocab size in reference grads from {wte_grad.size(0)} to {wte_grad_padded.size(0)}\")\n",
        "    with open(filename, \"wb\") as file:\n",
        "        # header\n",
        "        file.write(header.numpy().tobytes())\n",
        "        # input x\n",
        "        file.write(x.cpu().numpy().astype(\"int32\").tobytes()) # (B, T)\n",
        "        # targets y\n",
        "        file.write(y.cpu().numpy().astype(\"int32\").tobytes()) # (B, T)\n",
        "        # logits (result of the model forward pass)\n",
        "        write_fp32(logits.cpu(), file)\n",
        "        # loss (single float, result of the cross entropy loss)\n",
        "        write_fp32(loss.cpu(), file)\n",
        "        # gradients\n",
        "        write_tensors(grads, model.config.n_layer, file, \"float32\")\n",
        "    print(f\"wrote {filename}\")"
      ],
      "metadata": {
        "id": "ZekQNsh22A4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_tokenizer(enc, filename):\n",
        "    n = enc.max_token_value + 1\n",
        "    header = torch.zeros(256, dtype=torch.int32)\n",
        "    header[0] = 20240328 # magic\n",
        "    header[1] = 2 # tokenizer version = 2 (1 -> 2: includes EOT token)\n",
        "    header[2] = n # number of tokens\n",
        "    header[3] = enc.eot_token # EOT token\n",
        "    with open(filename, \"wb\") as file:\n",
        "        file.write(header.numpy().tobytes())\n",
        "        for i in range(n):\n",
        "            b = enc.decode_bytes([i])\n",
        "            length = len(b)\n",
        "            assert length < 256, f\"Token length exceeds 255: {length}\"\n",
        "            file.write(struct.pack(\"<B\", length))  # Write the length as a 1-byte unsigned integer\n",
        "            file.write(b)  # Write the actual bytes\n",
        "    print(f\"wrote {filename}\")"
      ],
      "metadata": {
        "id": "7_bhpRxa2DIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# int main"
      ],
      "metadata": {
        "id": "CH26AhAU2Edl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print0(*args, **kwargs):\n",
        "    # modified print that only prints from the master process\n",
        "    # if this is not a distributed run, it's just a print\n",
        "    if int(os.environ.get(\"RANK\", 0)) == 0:\n",
        "        print(*args, **kwargs)"
      ],
      "metadata": {
        "id": "ZV7eT0Z12HXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import time\n",
        "    import argparse\n",
        "    import tiktoken\n",
        "    print0(f\"Running pytorch {torch.version.__version__}\")\n",
        "\n",
        "    # default settings will overfit a tiny batch of data\n",
        "    # and save model weights and debug state to disk on the first iteration\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # file system input / output\n",
        "    parser.add_argument(\"--input_bin\", type=str, default=\"dev/data/tinyshakespeare/tiny_shakespeare_val.bin\", help=\"input .bin to train on\")\n",
        "    parser.add_argument(\"--input_val_bin\", type=str, default=\"\", help=\"input .bin to eval validation loss on\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"\", help=\"output directory to which to write logs and checkpoints\")\n",
        "    parser.add_argument(\"--model\", type=str, default=\"gpt2\", help=\"gpt2|gpt2-medium|gpt2-large|gpt2-xl|d12|d24|d36|d48\")\n",
        "    # token layout for each step of the optimization\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=4, help=\"batch size, in units of #batch dimensions\")\n",
        "    parser.add_argument(\"--sequence_length\", type=int, default=64, help=\"sequence length\")\n",
        "    parser.add_argument(\"--total_batch_size\", type=int, default=256, help=\"total desired batch size, in units of #tokens\")\n",
        "    # workload (number of steps)\n",
        "    parser.add_argument(\"--num_iterations\", type=int, default=10, help=\"number of iterations to run\")\n",
        "    parser.add_argument(\"--inference_only\", type=int, default=0, help=\"only run inference\")\n",
        "    # optimization\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=1e-4, help=\"learning rate warmup iterations\")\n",
        "    parser.add_argument(\"--warmup_iters\", type=int, default=0, help=\"learning rate warmup iterations\")\n",
        "    parser.add_argument(\"--learning_rate_decay_frac\", type=float, default=1.0, help=\"learning rate warmup iterations\")\n",
        "    parser.add_argument(\"--weight_decay\", type=float, default=0.0, help=\"weight decay\")\n",
        "    parser.add_argument(\"--grad_clip\", type=float, default=1.0, help=\"maximum gradient magnitude\")\n",
        "    # evaluation\n",
        "    parser.add_argument(\"--val_loss_every\", type=int, default=0, help=\"every how mant steps to evaluate val loss?\")\n",
        "    parser.add_argument(\"--val_max_steps\", type=int, default=20, help=\"how many batches of val to average?\")\n",
        "    parser.add_argument(\"--sample_every\", type=int, default=0, help=\"how often to sample from the model?\")\n",
        "    # debugging\n",
        "    parser.add_argument(\"--overfit_single_batch\", type=int, default=1, help=\"overfit just one batch of data\")\n",
        "    # numerics\n",
        "    parser.add_argument(\"--tensorcores\", type=int, default=0, help=\"use tensorcores\")\n",
        "    # memory management\n",
        "    parser.add_argument(\"--device\", type=str, default=\"\", help=\"by default we autodetect, or set it here\")\n",
        "    parser.add_argument(\"--compile\", type=int, default=0, help=\"torch.compile the model\")\n",
        "    parser.add_argument(\"--flash\", type=int, default=0, help=\"use flash attention\")\n",
        "    parser.add_argument(\"--dtype\", type=str, default=\"float32\", help=\"float32|float16|bfloat16\")\n",
        "    parser.add_argument(\"--zero_stage\", type=int, default=0, help=\"zero redundancy optimizer stage (0/1/2/3)\")\n",
        "    # python -> C bridge\n",
        "    parser.add_argument(\"--write_tensors\", type=int, default=1, help=\"write tensors to disk\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # args error checking and convenience variables\n",
        "    B, T = args.batch_size, args.sequence_length\n",
        "    assert 1 <= T <= 1024\n",
        "    assert args.dtype in {\"float32\", \"float16\", \"bfloat16\"}\n",
        "    assert args.model in {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\", \"d12\", \"d24\", \"d36\", \"d48\"}\n",
        "\n",
        "    # set up DDP (distributed data parallel). torchrun sets this env variable\n",
        "    ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "    if ddp:\n",
        "        # use of DDP atm demands CUDA, we set the device appropriately according to rank\n",
        "        assert torch.cuda.is_available(), \"for now i think we need CUDA for DDP\"\n",
        "        init_process_group(backend='nccl')\n",
        "        ddp_rank = int(os.environ['RANK'])\n",
        "        ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "        ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "        device = f'cuda:{ddp_local_rank}'\n",
        "        torch.cuda.set_device(device)\n",
        "        master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "        seed_offset = 0 # each process gets the exact same seed\n",
        "        zero_stage = args.zero_stage\n",
        "    else:\n",
        "        ddp_rank = 0\n",
        "        ddp_local_rank = 0\n",
        "        zero_stage = 0\n",
        "        ddp_world_size = 1\n",
        "        master_process = True\n",
        "        seed_offset = 0\n",
        "        # select the device\n",
        "        if args.device:\n",
        "            # provided explicitly by the user\n",
        "            device = args.device\n",
        "        else:\n",
        "            # attempt to autodetect the device\n",
        "            device = \"cpu\"\n",
        "            if torch.cuda.is_available():\n",
        "                device = \"cuda\"\n",
        "            elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "                device = \"mps\"\n",
        "    print(f\"using device: {device}\")\n",
        "    device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "\n",
        "    # calculate gradient accumulation from the desired total batch size and the current run configuration\n",
        "    tokens_per_fwdbwd = B * T * ddp_world_size\n",
        "    assert args.total_batch_size % tokens_per_fwdbwd == 0\n",
        "    grad_accum_steps = args.total_batch_size // tokens_per_fwdbwd\n",
        "    print0(f\"total desired batch size: {args.total_batch_size}\")\n",
        "    print0(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
        "\n",
        "    # set up a context manager following the desired dtype and device\n",
        "    ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[args.dtype]\n",
        "    ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()\n",
        "\n",
        "    # rng / reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(42)\n",
        "\n",
        "    # set the torch precision mode to use TensorFloat32 (TF32) for matmuls\n",
        "    # docs https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html\n",
        "    if args.tensorcores:\n",
        "        torch.set_float32_matmul_precision('high')\n",
        "\n",
        "    # turn on/off flash attention\n",
        "    assert args.flash in {0, 1}\n",
        "    FLASH = args.flash\n",
        "\n",
        "    # init (and write) the tokenizer\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    if master_process and args.write_tensors: # tokenizer is technically not tensors but ok\n",
        "        write_tokenizer(enc, \"gpt2_tokenizer.bin\")\n",
        "\n",
        "    # init the model, either from scratch or from OpenAI pretrained checkpoint\n",
        "    if args.model[0] == \"d\":\n",
        "        # from scratch (random weights)\n",
        "        model_config = {\n",
        "            \"d12\": GPTConfig(block_size=1024, vocab_size=50257, n_layer=12, n_head=12, n_embd=768),\n",
        "            \"d24\": GPTConfig(block_size=1024, vocab_size=50257, n_layer=24, n_head=16, n_embd=1024),\n",
        "            \"d36\": GPTConfig(block_size=1024, vocab_size=50257, n_layer=36, n_head=20, n_embd=1280),\n",
        "            \"d48\": GPTConfig(block_size=1024, vocab_size=50257, n_layer=48, n_head=25, n_embd=1600),\n",
        "        }[args.model]\n",
        "        model = GPT(model_config)\n",
        "    else:\n",
        "        # load the GPT-2 model weights\n",
        "        model = GPT.from_pretrained(args.model)\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    if args.compile:\n",
        "        if hasattr(config, \"coordinate_descent_tuning\"):\n",
        "            config.coordinate_descent_tuning = True # suggested by @Chillee\n",
        "        print0(\"compiling the model...\")\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Our own version of a simple DistributedDataLoader\n",
        "\n",
        "    # load tokens\n",
        "    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n",
        "    val_loader = None\n",
        "    if args.input_val_bin:\n",
        "        val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # PyTorch -> C bridge: save some weights and state for C to load later as reference\n",
        "\n",
        "    # do one forward pass to generate ground truth for our C tests\n",
        "    if master_process and args.write_tensors and (not args.inference_only):\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits, loss = model(x, y)\n",
        "        loss.backward()\n",
        "        # save model params, in both float32 and bfloat16\n",
        "        model_to_size = {\"gpt2\": \"124M\", \"gpt2-medium\": \"355M\", \"gpt2-large\": \"774M\", \"gpt2-xl\": \"1558M\"}\n",
        "        model_to_size.update({f\"d{d}\": f\"d{d}\" for d in [12, 24, 36, 48]})\n",
        "        model_size_str = model_to_size[args.model] # e.g. \"124M\", or \"d12\"\n",
        "        write_model(model, f\"gpt2_{model_size_str}.bin\", dtype=\"float32\")\n",
        "        write_model(model, f\"gpt2_{model_size_str}_bf16.bin\", dtype=\"bfloat16\")\n",
        "        # save x, y, logits, loss, and parameter gradients, for debugging C\n",
        "        # always store these in fp32 to have an accurate reference (?)\n",
        "        write_state(model, x, y, logits, loss, f\"gpt2_{model_size_str}_debug_state.bin\")\n",
        "        # reset the train_loader for the optimization below\n",
        "        train_loader.reset()\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # main training loop\n",
        "\n",
        "    # here we wrap model into DDP container\n",
        "    if ddp:\n",
        "        model = DDP(model, device_ids=[ddp_local_rank])\n",
        "    raw_model = model.module if ddp else model # always contains the \"raw\" unwrapped model\n",
        "\n",
        "    # init the optimizer\n",
        "    optimizer = raw_model.configure_optimizers(weight_decay=args.weight_decay,\n",
        "                                               learning_rate=args.learning_rate, betas=(0.9, 0.95),\n",
        "                                               device_type=device, zero_stage=zero_stage)\n",
        "\n",
        "    # learning rate decay scheduler (cosine with warmup)\n",
        "    def get_lr(it):\n",
        "        min_lr = args.learning_rate * args.learning_rate_decay_frac\n",
        "        # 1) linear warmup for warmup_iters steps\n",
        "        if it < args.warmup_iters:\n",
        "            return args.learning_rate * (it+1) / args.warmup_iters\n",
        "        # 2) if it > lr_decay_iters, return min learning rate\n",
        "        if it > args.num_iterations:\n",
        "            return min_lr\n",
        "        # 3) in between, use cosine decay down to min learning rate\n",
        "        decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n",
        "        assert 0 <= decay_ratio <= 1\n",
        "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
        "        return min_lr + coeff * (args.learning_rate - min_lr)\n",
        "\n",
        "    # create the logging directory if it does not exist\n",
        "    logfile = None\n",
        "    if args.output_dir:\n",
        "        os.makedirs(args.output_dir, exist_ok=True)\n",
        "        logfile = os.path.join(args.output_dir, \"main.log\")\n",
        "        # create the log file \"main.log\" inside it, and wipe it clean\n",
        "        with open(logfile, \"w\") as f:\n",
        "            pass\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "    timings = []\n",
        "    norm = -1.0   # dummy value to print in inference-only mode\n",
        "    for step in range(args.num_iterations + 1):\n",
        "        t0 = time.time()\n",
        "        last_step = (step == args.num_iterations)\n",
        "\n",
        "        # once in a while evaluate the validation dataset\n",
        "        if (args.val_loss_every > 0 \\\n",
        "            and (step % args.val_loss_every == 0 or last_step)) \\\n",
        "            and (val_loader is not None):\n",
        "            model.eval()\n",
        "            val_loader.reset()\n",
        "            with torch.no_grad():\n",
        "                val_loss = 0.0\n",
        "                for _ in range(args.val_max_steps):\n",
        "                    x, y = val_loader.next_batch()\n",
        "                    x, y = x.to(device), y.to(device)\n",
        "                    _, loss = model(x, y, return_logits=False)\n",
        "                    val_loss += loss.item()\n",
        "                val_loss /= args.val_max_steps\n",
        "            # log to console and to file\n",
        "            print0(f\"val loss {val_loss}\")\n",
        "            if master_process and logfile is not None:\n",
        "                with open(logfile, \"a\") as f:\n",
        "                    f.write(\"s:%d tel:%f\\n\" % (step, val_loss))\n",
        "\n",
        "        # once in a while perform model inference on the master process\n",
        "        if (args.sample_every > 0 \\\n",
        "            and (step % args.sample_every == 0 or last_step)) \\\n",
        "            and master_process:\n",
        "            model.eval()\n",
        "            # before we end, let's also do one round of inference\n",
        "            # we'll kick off the generation with \"<|endoftext|>\", which designates the start of a new sequence\n",
        "            start_ids = [enc.eot_token]\n",
        "            xg = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "            max_new_tokens = 32\n",
        "            temperature = 1.0\n",
        "            top_k = 40\n",
        "            yg = raw_model.generate(xg, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print0('---------------')\n",
        "            print0(enc.decode(yg[0].tolist()))\n",
        "            print0('---------------')\n",
        "\n",
        "        # bit confusing: we want to make sure to eval and sample on 0th iteration\n",
        "        # but also after the very last iteration. so we loop for step <= num_iterations\n",
        "        # instead of just < num_iterations (one extra due to <=), only to do\n",
        "        # the validation/sampling one last time, and then we break right here as we're done.\n",
        "        if last_step:\n",
        "            break\n",
        "\n",
        "        # --------------- TRAINING SECTION BEGIN -----------------\n",
        "        model.train()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        # if we are trying to overfit a single batch, we reset the loader here\n",
        "        if args.overfit_single_batch:\n",
        "            train_loader.reset()\n",
        "        # micro-batch loop where we do gradient accumulation to reach desired total batch size\n",
        "        lossf = 0.0 # for getting the mean loss (as simple float) over the accumulation steps\n",
        "        for micro_step in range(grad_accum_steps):\n",
        "            # fetch a batch\n",
        "            x, y = train_loader.next_batch()\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            if ddp:\n",
        "                # we want only the last micro-step to sync grads in a DDP model\n",
        "                # the official way to do this is with model.no_sync(), but that is a\n",
        "                # context manager that bloats the code, so we just toggle this variable\n",
        "                model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n",
        "            # forward pass\n",
        "            with ctx:\n",
        "                _, loss = model(x, y, return_logits=False)\n",
        "                # we have to scale the loss to account for gradient accumulation,\n",
        "                # because the gradients just add on each successive backward().\n",
        "                # addition of gradients corresponds to a SUM in the objective, but\n",
        "                # instead of a SUM we want MEAN, so we scale the loss here\n",
        "                loss = loss / grad_accum_steps\n",
        "                lossf += loss.detach() # keep track of the mean loss\n",
        "            # backward pass\n",
        "            if not args.inference_only:\n",
        "                loss.backward()\n",
        "        if ddp:\n",
        "            dist.all_reduce(lossf, op=dist.ReduceOp.AVG)\n",
        "        lossf = lossf.item()\n",
        "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
        "        # determine and set the learning rate for this iteration\n",
        "        lr = get_lr(step)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        # step the optimizer\n",
        "        optimizer.step()\n",
        "        # --------------- TRAINING SECTION END -------------------\n",
        "        # everything that follows now is just diagnostics, prints, logging, etc.\n",
        "\n",
        "        # wait on the CPU for all device work to end so we get accurate per-iteration timings below\n",
        "        if device == \"mps\":\n",
        "            torch.mps.synchronize()\n",
        "        elif device == \"cuda\":\n",
        "            torch.cuda.synchronize()\n",
        "        # time and print\n",
        "        t1 = time.time()\n",
        "        # the 0th iteration is often an outlier (much slower) => skip logging it\n",
        "        tokens_per_second = grad_accum_steps * ddp_world_size * B * T / (t1-t0)\n",
        "        print0(f\"step {step+1:4d}/{args.num_iterations} | train loss {lossf:.6f} | norm {norm:.4f} | lr {lr:.2e} | ({(t1-t0)*1000:.2f} ms | {tokens_per_second:.0f} tok/s)\")\n",
        "        # log to logile\n",
        "        if master_process and logfile is not None:\n",
        "            with open(logfile, \"a\") as f:\n",
        "                f.write(\"s:%d trl:%f\\n\" % (step, lossf))\n",
        "\n",
        "        # keep track of smooth timings, last 20 iterations\n",
        "        if step > 0 and step > args.num_iterations - 20:\n",
        "            timings.append(t1-t0)\n",
        "\n",
        "    # print the average of the last 20 timings, to get something smooth-ish\n",
        "    timings = timings[-20:]\n",
        "    print0(f\"final {len(timings)} iters avg: {np.mean(timings)*1000:.3f}ms\")\n",
        "    print0(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # clean up nice\n",
        "    if ddp:\n",
        "        destroy_process_group()"
      ],
      "metadata": {
        "id": "WERtaSpT2HtM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}