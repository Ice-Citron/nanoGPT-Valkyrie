{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 8 Aug 2024\n",
        "\n",
        "https://chatgpt.com/c/a9b218dc-221f-4f6c-8fa2-9b6c78c07ab5"
      ],
      "metadata": {
        "id": "Pw500VQNzTQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The image you provided outlines the algorithm for Batch Normalization specifically used in a Transformer model. Here's a breakdown of the equations and their meaning:\n",
        "\n",
        "### Forward Propagation\n",
        "1. **Inputs and Outputs**:\n",
        "   - **X**: The input matrix of shape \\( R^{B \\times d} \\), where \\( B \\) is the batch size and \\( d \\) is the feature dimension.\n",
        "   - **Y**: The output matrix after batch normalization.\n",
        "\n",
        "2. **Mini-batch Mean (\\( \\mu_B \\))**:\n",
        "   - Calculated as the mean of the inputs across the batch.\n",
        "   - \\( \\mu_B = \\frac{1}{B} \\sum_{i=1}^B x_i \\) where \\( x_i \\) are the input vectors in the batch.\n",
        "\n",
        "3. **Mini-batch Variance (\\( \\sigma_B^2 \\))**:\n",
        "   - Calculated as the variance of the inputs within the batch.\n",
        "   - \\( \\sigma_B^2 = \\frac{1}{B} \\sum_{i=1}^B (x_i - \\mu_B)^2 \\)\n",
        "\n",
        "4. **Normalization**:\n",
        "   - \\( \\hat{X} = \\frac{X - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\) where \\( \\epsilon \\) is a small number added for numerical stability (not shown in the image).\n",
        "\n",
        "5. **Scale and Shift**:\n",
        "   - The normalized data is scaled and shifted using parameters \\( \\gamma \\) and \\( \\beta \\), which are learned during training.\n",
        "   - \\( Y = \\gamma \\hat{X} + \\beta \\)\n",
        "\n",
        "### Updating Parameters During Training\n",
        "- **Running Mean (\\( \\mu \\))**:\n",
        "  - Updated with a momentum \\( \\alpha \\), blending the current mini-batch mean and the previous running mean.\n",
        "  - \\( \\mu = \\alpha \\mu + (1 - \\alpha)\\mu_B \\)\n",
        "\n",
        "- **Running Variance (\\( \\sigma^2 \\))**:\n",
        "  - Similarly updated using the current mini-batch variance.\n",
        "  - \\( \\sigma^2 = \\alpha \\sigma^2 + (1 - \\alpha)\\sigma_B^2 \\)\n",
        "\n",
        "### Backward Propagation (used during training)\n",
        "- **Inputs**:\n",
        "  - \\( \\frac{\\partial L}{\\partial Y} \\) (the gradient of the loss with respect to the output \\( Y \\)).\n",
        "- **Outputs**:\n",
        "  - \\( \\frac{\\partial L}{\\partial X} \\) (the gradient of the loss with respect to the input \\( X \\)).\n",
        "- These gradients are used to update \\( \\gamma \\) and \\( \\beta \\) and are calculated based on further equations not shown here but referred to as Equation 3.\n",
        "\n",
        "### Inference\n",
        "- Once the model is trained, the normalization uses the learned running mean and variance:\n",
        "  - \\( Y = \\gamma \\frac{X - \\mu}{\\sigma} + \\beta \\)\n",
        "\n",
        "This algorithm helps stabilize the learning process by normalizing the inputs and making the model less sensitive to the scale of different features. Batch Normalization is particularly effective in deep networks and can lead to faster convergence and higher overall accuracy."
      ],
      "metadata": {
        "id": "_hk9d8AWzTJT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGLxIEbQzR0L"
      },
      "outputs": [],
      "source": []
    }
  ]
}